{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n        \nprint('\\n\\n\\n\\n\\n')\nfor dirname, _, filenames in os.walk('/kaggle/output'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:17:52.830767Z","iopub.execute_input":"2022-04-11T14:17:52.831113Z","iopub.status.idle":"2022-04-11T14:17:52.968880Z","shell.execute_reply.started":"2022-04-11T14:17:52.831025Z","shell.execute_reply":"2022-04-11T14:17:52.967907Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!mv ../input/spamdetectionidea1and2epoch50model/loss_framework-2_Spam-Detection_.png ../input/spam-detection-idea-1-and-2-epoch-50-model","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:17:52.970656Z","iopub.execute_input":"2022-04-11T14:17:52.971250Z","iopub.status.idle":"2022-04-11T14:17:53.741505Z","shell.execute_reply.started":"2022-04-11T14:17:52.971204Z","shell.execute_reply":"2022-04-11T14:17:53.740379Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# **BUILDING MODEL IN OOPS WAY FOR BOTH SPAM/INAPP CONTENT DETECTION**\nThus this helps in providing abstraction and easy way to deploy model as well as will also help in providing an machine learning API for our proposed work. ","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"#Importing all the libraries \n\nprint(\"WORKING ON ML Models in object oriented way....\")\n\nimport time\nstart_time = time.time()\n# from numpy import asarray\n# from numpy import loadtxt\n# from numpy import savetxt\nimport pickle\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd, matplotlib.pyplot as plt \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras.layers import InputSpec\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n# from keras.optimizers import  RMSprop, adam_v2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport_time = round((time.time()-start_time)*1000,3)\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\n\n\nprint(f\"'\\n\\nALL LIBRARIES IMPORTED SUCCESSFULLY!!' in TIME : {import_time} msec\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:17:53.744358Z","iopub.execute_input":"2022-04-11T14:17:53.744922Z","iopub.status.idle":"2022-04-11T14:18:00.231615Z","shell.execute_reply.started":"2022-04-11T14:17:53.744866Z","shell.execute_reply":"2022-04-11T14:18:00.230418Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    '''\n     Customized Class to handle ROC AUC Evaluation for each epoch and printing epoch number and score for each epoch which is multiple of interval.  \n    '''\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n\n#################################################################################################################################################\n\nclass Modelbase :\n    def __init__(self, embed_size = 300, max_features = 130000, max_len = 220, model_Type = \"Spam-Detection\", *args, **kwargs,):\n        self.model_Type = model_Type\n        self.embed_size = embed_size\n        self.max_features = max_features #Vocabulary Size \n        self.max_len = max_len  # maximum length of tweet. \n        self.X_train = None\n        self.Y_train = None\n        self.X_valid = None\n        self.Y_valid = None\n        self.train = None\n        self.y = None\n        self.raw_text_train = None \n        self.raw_text_valid= None\n        if self.model_Type == \"Spam-Detection\":\n            self.list_classes = [\"ham\", \"spam\"]\n            self.tweet_column = \"Message\"\n            self.train_df1 = None\n            self.train_df2 = None            \n        else:\n            self.list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n            self.tweet_column = \"comment_text\"\n            self.test = None\n            self.raw_test_valid= None        \n            self.X_test =  None\n            \n        \n    def read_dataset(self,filepath):\n        '''\n        Reading the training and testing data i.e converting that into dataframes\n        input : file path \n        ----------------\n        output : dataframe \n        '''\n        df = pd.read_csv(filepath)\n        return df \n    \n    def build_train_test_dev_set(self, \n                                 trainDF1 = \"../input/spam-or-not-spam-dataset/spam_or_not_spam.csv\" , \n                                 trainDF2 = \"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\" , \n                                 trainDF  = \"../input/jigsaw-toxic-comment-classification-challenge/train.csv\" , \n                                 testDF   = \"../input/jigsaw-toxic-comment-classification-challenge/test.csv\"  ):\n        \n        if self.model_Type == \"Spam-Detection\"  :\n            self.train_df1 = self.read_dataset(trainDF1)\n            self.train_df2 = self.read_dataset(trainDF2)\n            self.spam_Dataset_Aggregator()\n            self.train = self.train.dropna()\n            return \n        else:    \n            self.train  = self.read_dataset(trainDF)\n            self.test  = self.read_dataset(testDF)\n#             self.train = self.train.dropna()\n            return\n        \n    def spam_Dataset_Aggregator(self):\n        self.train_df1.rename( columns={self.train_df1.columns[0] : \"Message\", self.train_df1.columns[1] : \"Category\" },inplace=True)\n        self.train_df2[self.train_df2.columns[0]].replace({\"ham\": 0, \"spam\": 1}, inplace=True)\n        self.train_df2 = self.train_df2[ [ self.train_df2.columns[1], self.train_df2.columns[0] ] ]\n        self.train = pd.concat([self.train_df1, self.train_df2], ignore_index=True, sort=False)\n        return  \n        \n    def __repr__(self):\n        if self.model_Type == \"Spam-Detection\":\n            return f\"Model for Spam Detection (Ham/Spam detection) \\n Model Type :  {self.model_Type}\"\n        return f\"Model for Inappropraue Content Detection (Ham/Spam detection) \\n Model Type :  {self.model_Type}\" #Inappropriate-Content-Detection\n\n    \n    \n#######################################################################################################################################################\n\n\n\nclass Preprocessor(Modelbase):\n    \n    def __init__(self,embed_size = 300, \n                 max_features = 130000, \n                 max_len = 220, \n                 model_Type = \"Spam-Detection\", \n                 *args, **kwargs):\n#         super(Model, self).__init__()\n        super().__init__(embed_size , max_features, max_len, model_Type)\n        # num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n        self.tk = Tokenizer(num_words = max_features)\n        self.embedding_matrix_fastText = None\n        self.embedding_matrix_glove = None\n        self.embedding_index_fastText = None\n        self.embedding_index_glove = None\n        self.embedding_matrix_fastText_shape = None\n        self.embedding_matrix_glove_shape = None\n#         self.embedding_index_fastText_shape = None\n#         self.embedding_index_glove_shape = None\n        \n    def train_test_splitter(self,validation_size=0.1):\n        '''\n        Input : training dataset Dataframe -> train\n        ------------\n        Output : Training and dev dataframe with X and Y values along with raw train and dev set with all comments in lower.\n        '''\n        if self.model_Type != \"Spam-Detection\" :\n            # y = np.asarray(y).astype(np.float32)\n            self.y = self.train[self.list_classes].values\n            self.train[self.tweet_column].fillna(\"no comment\")\n            self.test[self.tweet_column].fillna(\"no comment\") \n#             self.train = self.train.dropna()  #**********************************************************\n        else:\n            self.y = self.train[\"Category\"].values\n            # y = np.asarray(y).astype(np.float32)\n            self.train[self.tweet_column].fillna(\"no comment\")\n        # train_df2[\"Message\"].fillna(\"no comment\")\n        validation_size = validation_size   #by default it is 0.1\n        # splitting train:dev -->  9:1 so train consist of 90 percent and validation set consist of 10 percent of entire training data \n        #Note : This validation_size is configurable and can be changed later.\n        self.X_train, self.X_valid, self.Y_train, self.Y_valid = train_test_split(self.train, self.y, test_size = validation_size, random_state=512)\n        # Lowering all the comments of training, validation and test data let callled it as raw.\n        self.raw_text_train = self.X_train[self.tweet_column].str.lower()\n        self.raw_text_valid = self.X_valid[self.tweet_column].str.lower()\n        if self.model_Type != \"Spam-Detection\" :\n            self.raw_test_valid = self.test[self.tweet_column].str.lower()\n        return\n\n    def tokenizer(self):\n            '''\n            Input : raw train and dev set along with in-rawed train and dev set.\n            --------------------------------------------------------------------------------\n            Output: tokenizer object along with padded and sequenced training and validation dataset\n            '''\n            # Article for better undrstanding of Tokenizer : https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/\n            #Tokeinzing the raw training set\n            self.tk.fit_on_texts(self.raw_text_train)\n            print(self.raw_text_train.shape )\n            self.X_train[self.tweet_column] = self.tk.texts_to_sequences(self.raw_text_train)\n            self.X_valid[self.tweet_column] = self.tk.texts_to_sequences(self.raw_text_valid)\n            self.X_train = pad_sequences(self.X_train[self.tweet_column], maxlen = self.max_len)\n            self.X_valid = pad_sequences(self.X_valid[self.tweet_column], maxlen = self.max_len)\n            if self.model_Type != \"Spam-Detection\" :\n                self.test[self.tweet_column] = self.tk.texts_to_sequences(self.raw_test_valid )\n                self.test = pad_sequences(self.test[self.tweet_column], maxlen = self.max_len) #test.comment_seq\n            return\n    \n    def get_coefs(self,word,*arr): \n        '''\n        Creating Embeeding Index which can help further to create embedding matrix for the words in our training dataset vocabulary.\n        This Ebedding index is created from the fastText or Glove.\n        '''\n        return word, np.asarray(arr, dtype='float32')\n    \n    \n    def embeeding_Index_Builder(self, embedding_path):\n        '''\n        Embedding Index correpsonding to embeddding Path\n        Input : embeddig path \n        ----------------------\n        Output : embedding Index\n        '''\n        embedding_index = dict(self.get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n        return embedding_index\n\n    def embeeding_Matrix_Builder(self,embedding_index ):\n        '''\n        Input : tokenizer object , with maximum features and embedding size along with fastText and Glove Embedding Index for building Embeeding Matrix\n        ----------------------------------------------------------------------------------------------------------------------------------------------\n        Output: embedding matix corresponding to FastText and Glove\n        '''\n        # Preparing Our Embeeding matrix from Embedding Index from glove or fastText or any other index. \n        damword_index = self.tk.word_index\n        nb_words = min(self.max_features, len(damword_index))\n        embedding_matrix = np.zeros((nb_words+1, self.embed_size))\n        for word, i in damword_index.items():\n            if i >= self.max_features: continue\n            embedding_vector = embedding_index.get(word)\n            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        return embedding_matrix\n    \n    def save_embeeding_index_or_matrix_as_csv(self, embedding_index_or_matrix, arrayType=\"index\", embeddingType=\"FastText\"):\n        # save numpy array embeeding_index as csv file\n        # define data\n        if arrayType!=\"index\":\n            data = copy.deepcopy(embedding_index_or_matrix)\n            # save to csv file\n            data = data.ravel()\n            np.savetxt(f'embedding_{arrayType}_{embeddingType}.csv', data, delimiter=',')\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"wb\")\n            pickle.dump(embedding_index_or_matrix, a_file)\n            a_file.close() \n        \n    def load_embedding_index_or_matrix_from_csv(self, arrayType=\"index\", embeddingType=\"FastText\"):\n        # load numpy array  embeeding_index from csv file\n        # load array\n        if arrayType!=\"index\":\n            data = np.loadtxt(f'embedding_{arrayType}_{embeddingType}.csv', delimiter=',')\n            reshaped_data = None\n\n            if embeddingType==\"FastText\" :\n#                 if arrayType==\"index\":\n                reshaped_data = np.reshape(data,self.embedding_index_fastText_shape )\n#                 else:\n#                     reshaped_data = np.reshape(data,self.embedding_matrix_fastText_shape )\n            elif embeddingType==\"Glove\":\n#                 if arrayType==\"index\":\n                reshaped_data = np.reshape(data,self.embedding_index_glove_shape )\n#                 else:\n#                     reshaped_data = np.reshape(data,self.embedding_matrix_glove_shape )\n            # print the array\n            return reshaped_data\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"rb\")\n            output = pickle.load(a_file)\n            return output\n\n            \n    \n    def save_embeeding_index_or_matrix_as_binary(self, embedding_index_or_matrix , arrayType=\"index\", embeddingType=\"FastText\"):\n        # save numpy array embeeding_index as binary file npy\n        # define data\n        if arrayType!=\"index\":\n            np.save(f'embedding_{arrayType}_{embeddingType}.npy', embedding_index_or_matrix )\n            return\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"wb\")\n            pickle.dump(embedding_index_or_matrix, a_file)\n            a_file.close() \n        \n    def load_embedding_index_or_matrix_from_binary(self, arrayType=\"index\", embeddingType=\"FastText\"):\n        # load numpy array  embeeding_index from npy file\n        # load array\n        if arrayType!=\"index\":\n            data = np.load(f'embedding_{arrayType}_{embeddingType}.csv')\n            return data\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"rb\")\n            output = pickle.load(a_file)\n            return output\n\n\n    def preprocessing(self, \n                      validation_size=0.1,\n                      embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n                      embedding_path_glove =\"../input/glove840b300dtxt/glove.840B.300d.txt\",\n                      isEmbeddingIndexFileSaved     =False,\n                      isEmbeddingMatrixFileSaved    = False,\n                      wantToSaveEmbeedingIndexFile  = False,\n                      wantToSaveEmbeedingMatrixFile = False ):\n        self.build_train_test_dev_set()\n        print('Building Trainning and Testing is Done.')\n        self.train_test_splitter(validation_size)\n        print('Spliting Trainning and Testing is Done.')\n        self.tokenizer()\n        print('Tokenizing is Done.')\n        if not isEmbeddingIndexFileSaved :\n            self.embedding_index_fastText = self.embeeding_Index_Builder(embedding_path_fastText)\n            print('Embeeding Index for Fasttext is Done.')\n            self.embedding_index_glove = self.embeeding_Index_Builder(embedding_path_glove)\n            print('Embeeding Index for Glove is Done.')\n            if wantToSaveEmbeedingIndexFile:\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_index_fastText  , arrayType=\"index\", embeddingType=\"FastText\")\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_index_glove  , arrayType=\"index\", embeddingType=\"Glove\")\n        else:\n            self.embedding_index_fastText  = self.load_embedding_index_or_matrix_from_binary(arrayType=\"index\", embeddingType=\"FastText\")\n            self.embedding_index_glove  = self.load_embedding_index_or_matrix_from_binary(arrayType=\"index\", embeddingType=\"Glove\")\n#         self.embedding_index_fastText_shape = self.embedding_index_fastText.shape\n#         self.embedding_index_glove_shape = self.embedding_index_glove.shape\n        \n        if not isEmbeddingMatrixFileSaved :\n            self.embedding_matrix_fastText = self.embeeding_Matrix_Builder( self.embedding_index_fastText )\n            print(f'Embedding Matrix for FastText is Done., with it\\'s shape as {self.embedding_matrix_fastText_shape}')\n            self.embedding_matrix_glove = self.embeeding_Matrix_Builder( self.embedding_index_glove )\n            print(f'Embedding Matrix for Glove is Done with shape as { self.embedding_matrix_glove_shape}')\n            if  wantToSaveEmbeedingMatrixFile:\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_matrix_fastText , arrayType=\"matrix\", embeddingType=\"FastText\")\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_matrix_glove   , arrayType=\"matrix\", embeddingType=\"Glove\")\n        else:\n            self.embedding_matrix_fastText  = self.load_embedding_index_or_matrix_from_binary(arrayType=\"matrix\", embeddingType=\"FastText\")\n            self.embedding_matrix_glove     = self.load_embedding_index_or_matrix_from_binary(arrayType=\"matrix\", embeddingType=\"Glove\")\n#         self.embedding_matrix_fastText_shape = self.embedding_matrix_fastText.shape\n#         self.embedding_matrix_glove_shape =  self.embedding_matrix_glove.shape \n        return\n    \n#######################################################################################################################################################\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:18:00.233829Z","iopub.execute_input":"2022-04-11T14:18:00.234406Z","iopub.status.idle":"2022-04-11T14:18:00.293073Z","shell.execute_reply.started":"2022-04-11T14:18:00.234362Z","shell.execute_reply":"2022-04-11T14:18:00.291947Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class ML_Model :\n    def __init__(self, \n                 ModelBaseInstance = Preprocessor( embed_size = 300, \n                                           max_features = 130000, \n                                           max_len = 220, \n                                           model_Type = \"Spam-Detection\"),\n#                  embed_size = 300, \n#                  max_features = 130000, \n#                  max_len = 220, \n#                  model_Type = \"Spam-Detection\", \n                 file_path = \"1\",\n                 n_multiClassificationClasses = 1 ,# if Inappropriate then 6 as they are : \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"\n                 *args, **kwargs):  # for file_path just pass which idea/framework on which this Model is based upon\n#         super(Preprocessor, self).__init__()\n#         super().__init__( embed_size = 300, \n#                           max_features = 130000, \n#                           max_len = 220, \n#                           model_Type = \"Spam-Detection\")\n        self.modelBase = ModelBaseInstance\n        self.model = None\n        self.history = None\n        self.framework_idea = file_path \n        self.file_path = \"best_model_\" + \"FrameWork\" + self.framework_idea + \"_\" + self.modelBase.model_Type  + \".hdf5\" \n        self.check_point = ModelCheckpoint(self.file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n        self.ra_val = None\n        #Allowing early stopping\n        self.early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n        self.n_output_nuerons = n_multiClassificationClasses\n        \n    def change_idea_for_same_object(self,change_idea_to=\"2\", automatic_train = False, *args, **kwargs):\n        earlier_idea = self.framework_idea \n        self.framework_idea = change_idea_to\n        self.file_path = \"best_model_\" + \"FrameWork\" + self.framework_idea + \"_\" + self.modelBase.model_Type + \".hdf5\" \n        self.check_point = ModelCheckpoint(self.file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n        print(f\"Framework is changeed succesfully \\n Idea/Frameowkr earlier :  {earlier_idea} \\n Idea/Framework Now {self.framework_idea } : \")\n        print(\"\\n\\n PS: This method will keep in handy when you want to test for multiple idea without doing preprocessing step again.\")\n        # TO-DO  allo training auto to be done for user given parameters\n        if automatic_train:\n            print(f\"Note: The training is requested to be done AUTOMATICALLY as automatic_train flag is {automatic_train}, Note:  This training will be done on default parameters \\n i.e lr = 0.0, \\n lr_d = 0.0, \\n units = 0, \\n dr = 0.0, \\n epochs=10.\")\n            print('OR The parameters which was set for earlier IDEA/FRAMEWORK')\n            self.train_Model()\n        else:\n            print(\"\\t Note : WE NEED TO RE-TRAINED THE MODEL.\\n\\n \\t For Help :: USE METHOD --> train_Model for this in order to train the model on new Idea/Framework. \\n \\t Not training automatically as Automatic train Flag is off.\")\n        return\n    \n    def roc_Auc_Valuation(self):\n        self.ra_val = RocAucEvaluation(validation_data=(self.modelBase.X_valid, self.modelBase.Y_valid), interval = 1)\n        return\n        \n    def build_model_framework1(self, lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n        inp = Input(shape = (self.modelBase.max_len,))\n#         x_fastText = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n#         x_glove = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n        x_fastText = Embedding(self.modelBase.embedding_matrix_fastText.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n        x_glove = Embedding(self.modelBase.embedding_matrix_glove.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n        \n    \n    #Drop-Out Layer\n        x1_fastText = SpatialDropout1D(dr)(x_fastText)\n        x1_glove = SpatialDropout1D(dr)(x_glove)\n        #BI-LSTM BRANCH\n        x_glove_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #BI-GRU BRNACH\n        x_glove_gru = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_gru = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #Convolutional+Pooling Layer This is Optional Can be commnented later if required for testing purposes\n        #Bi-listm\n        x_glove_lstm_conv           = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_lstm)\n        x_glove_lstm_conv_avgPool   = GlobalAveragePooling1D()(x_glove_lstm_conv)\n        x_glove_lstm_conv_maxPool   = GlobalMaxPooling1D()(x_glove_lstm_conv)\n        #concatenating\n        x_glove_lstm_conv_pooled    = concatenate([x_glove_lstm_conv_avgPool, x_glove_lstm_conv_maxPool])\n\n        x_fastText_lstm_conv        = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_lstm)\n        x_fastText_lstm_conv_avgPool= GlobalAveragePooling1D()(x_fastText_lstm_conv)\n        x_fastText_lstm_conv_maxPool= GlobalMaxPooling1D()(x_fastText_lstm_conv)\n        #concatenating\n        x_fastText_lstm_conv_pooled = concatenate([x_fastText_lstm_conv_avgPool, x_fastText_lstm_conv_maxPool])\n        #Bi-gru\n        x_glove_gru_conv            = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_gru)\n        x_glove_gru_conv_avgPool    = GlobalAveragePooling1D()(x_glove_gru_conv)\n        x_glove_gru_conv_maxPool    = GlobalMaxPooling1D()(x_glove_gru_conv)\n        #concatenating\n        x_glove_gru_conv_pooled     = concatenate([x_glove_gru_conv_avgPool, x_glove_gru_conv_maxPool])\n        x_fastText_gru_conv         = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_gru)\n        x_fastText_gru_conv_avgPool = GlobalAveragePooling1D()(x_fastText_gru_conv)\n        x_fastText_gru_conv_maxPool = GlobalMaxPooling1D()(x_fastText_gru_conv)\n        #concatenating\n        x_fastText_gru_conv_pooled  = concatenate([x_fastText_gru_conv_avgPool, x_fastText_gru_conv_maxPool])\n        #Con-Catenating Bi-Lstm branch\n        x_lstm_conv_pooled = concatenate([x_glove_lstm_conv_pooled,  x_fastText_lstm_conv_pooled])\n        #Con-Catenating Bi-Gru branch\n        x_gru_conv_pooled = concatenate([x_glove_gru_conv_pooled,  x_fastText_gru_conv_pooled])\n        #ConCatenating Bi-LSTM and Bi-GRU Branch\n        x_lstm_gru_concatenated = concatenate([x_lstm_conv_pooled, x_gru_conv_pooled])\n        #Passing concatenated output to dense network\n    #     xxxx = Dense(8, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n    #     xxx = Dense(4, activation = \"sigmoid\")(xxxx)\n    #     xx = Dense(2, activation = \"sigmoid\")(xx)\n    #     x = Dense(1, activation = \"sigmoid\")(xx) \n        \n#         if self.model_Type != \"Spam-Detection\" :\n#             assert self.n_output_nuerons==6\n#         else:\n#             assert self.n_output_nuerons==1\n\n        print(self.n_output_nuerons)\n            \n        x = Dense(self.n_output_nuerons, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n        self.model = Model(inputs = inp, outputs = x)\n        self.model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    #     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    \n#         print(\"*************************************************\")\n#         print(f\"Y_train's size/shape  ={self.modelBase.Y_train.shape} \")\n#         print(f\"Y_valid's size/shape  ={self.modelBase.Y_valid.shape} \")  \n        \n        self.history = self.model.fit(self.modelBase.X_train, self.modelBase.Y_train, batch_size = 128, epochs = epochs, validation_data = (self.modelBase.X_valid, self.modelBase.Y_valid), \n                            verbose = 1, callbacks = [self.ra_val, self.check_point, self.early_stop])\n        self.model = load_model(self.file_path)\n        return\n        \n    def build_model_framework2(self,lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n        inp = Input(shape = (self.modelBase.max_len,))\n#         x_fastText = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n#         x_glove = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n\n        x_fastText = Embedding(self.modelBase.embedding_matrix_fastText.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n        x_glove = Embedding(self.modelBase.embedding_matrix_glove.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n       \n    #Drop-Out Layer\n        x1_fastText = SpatialDropout1D(dr)(x_fastText)\n        x1_glove = SpatialDropout1D(dr)(x_glove)\n        #BI-LSTM BRANCH\n        x_glove_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #BI-GRU BRNACH\n        x_glove_gru = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_gru = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #Convolutional+Pooling Layer This is Optional Can be commnented later if required for testing purposes\n        #Bi-listm\n        x_glove_lstm_conv           = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_lstm)\n        x_glove_lstm_conv_avgPool   = GlobalAveragePooling1D()(x_glove_lstm_conv)\n        x_glove_lstm_conv_maxPool   = GlobalMaxPooling1D()(x_glove_lstm_conv)\n        #concatenating\n        x_glove_lstm_conv_pooled    = concatenate([x_glove_lstm_conv_avgPool, x_glove_lstm_conv_maxPool])\n\n        x_fastText_lstm_conv        = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_lstm)\n        x_fastText_lstm_conv_avgPool= GlobalAveragePooling1D()(x_fastText_lstm_conv)\n        x_fastText_lstm_conv_maxPool= GlobalMaxPooling1D()(x_fastText_lstm_conv)\n        #concatenating\n        x_fastText_lstm_conv_pooled = concatenate([x_fastText_lstm_conv_avgPool, x_fastText_lstm_conv_maxPool])\n        #Bi-gru\n        x_glove_gru_conv            = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_gru)\n        x_glove_gru_conv_avgPool    = GlobalAveragePooling1D()(x_glove_gru_conv)\n        x_glove_gru_conv_maxPool    = GlobalMaxPooling1D()(x_glove_gru_conv)\n        #concatenating\n        x_glove_gru_conv_pooled     = concatenate([x_glove_gru_conv_avgPool, x_glove_gru_conv_maxPool])\n\n        x_fastText_gru_conv         = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_gru)\n        x_fastText_gru_conv_avgPool = GlobalAveragePooling1D()(x_fastText_gru_conv)\n        x_fastText_gru_conv_maxPool = GlobalMaxPooling1D()(x_fastText_gru_conv)\n        #concatenating\n        x_fastText_gru_conv_pooled  = concatenate([x_fastText_gru_conv_avgPool, x_fastText_gru_conv_maxPool])\n        #Con-Catenating Bi-Lstm branch\n        x_lstm_conv_pooled = concatenate([x_glove_lstm_conv_pooled,  x_fastText_lstm_conv_pooled])\n        #Con-Catenating Bi-Gru branch\n        x_gru_conv_pooled = concatenate([x_glove_gru_conv_pooled,  x_fastText_gru_conv_pooled])\n        #ConCatenating Bi-LSTM and Bi-GRU Branch\n        x_lstm_gru_concatenated = concatenate([x_lstm_conv_pooled, x_gru_conv_pooled])\n        #Passing concatenated output to dense network\n    #     xxxx = Dense(8, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n    #     xxx = Dense(4, activation = \"sigmoid\")(xxxx)\n    #     xx = Dense(2, activation = \"sigmoid\")(xxx)\n    #     x = Dense(1, activation = \"sigmoid\")(xx)\n    \n        print(self.n_output_nuerons)\n\n        x = Dense(self.n_output_nuerons, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n        self.model = Model(inputs = inp, outputs = x)\n        self.model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    #     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n        self.history = self.model.fit(self.modelBase.X_train, self.modelBase.Y_train, batch_size = 128, epochs = epochs, validation_data = (self.modelBase.X_valid, self.modelBase.Y_valid), \n                            verbose = 1, callbacks = [self.ra_val, self.check_point, self.early_stop])\n        self.model = load_model(self.file_path)\n        return \n    \n    def build_model_framework3(self,lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n        pass\n    \n    def build_model(self, lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 30):\n        self.roc_Auc_Valuation()\n        if self.framework_idea == \"1\":\n            self.build_model_framework1( lr, lr_d , units, dr ,epochs)\n        elif self.framework_idea == \"2\":\n            self.build_model_framework2( lr, lr_d , units, dr ,epochs)\n        else : \n            self.build_model_framework3( lr, lr_d , units, dr ,epochs)\n        \n    def train_Model(self, lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 30):\n        #For training use this member function\n        print(\"TRAINING STARTED\")\n        self.build_model(lr , lr_d, units, dr ,epochs)\n        print(\"TRAINING COMPLETED\")\n    \n    def get_accuracy_for_validation_set(self):\n        return self.prediction(self.modelBase.X_valid, self.modelBase.Y_valid)\n        \n    def get_accuracy_for_training_set(self):\n        return self.prediction(self.modelBase.X_train, self.modelBase.Y_train)\n         \n    def Plot(self, string): # example Object.Plot(history, \"accuracy\")  or Object.Plot(history, \"loss\")\n        plt.plot(self.history.history[string])\n        plt.plot(self.history.history['val_' + string])\n        plt.xlabel(\"EPOCHS\")\n        plt.ylabel(string)\n        plt.legend([string, 'val_' + string ])\n        plt.savefig(string + \"_framework-\" + self.framework_idea  +'_' + self.modelBase.model_Type  + '_'+'.png')\n        plt.show()       \n        print(f\"###### DONE PLOTTING FOR IDEA - {self.framework_idea} ########\")\n                    \n    def prediction(self,dataset, y_actual,on_the_fly=0 ): # On the fly is for those data that is comimg on the fly from any tweet.\n        y_pred_validation = self.model.predict(dataset, verbose=1)\n        score_validation = roc_auc_score(y_actual, y_pred_validation)\n        print(f\"\\n ROC-AUC - ON given Dataset - score: {round(score_validation,5)*100}%\")\n        return y_pred_validation\n        \n##############################################################################################################################################################","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:18:00.299637Z","iopub.execute_input":"2022-04-11T14:18:00.299856Z","iopub.status.idle":"2022-04-11T14:18:00.356376Z","shell.execute_reply.started":"2022-04-11T14:18:00.299826Z","shell.execute_reply":"2022-04-11T14:18:00.355379Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# random_state=None","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:18:00.358521Z","iopub.execute_input":"2022-04-11T14:18:00.359222Z","iopub.status.idle":"2022-04-11T14:18:00.369962Z","shell.execute_reply.started":"2022-04-11T14:18:00.359159Z","shell.execute_reply":"2022-04-11T14:18:00.369006Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"#      validation_size=0.1,\n#                       embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n#                       embedding_path_glove =\"../input/glove840b300dtxt/glove.840B.300d.txt\",\n#                       isEmbeddingIndexFileSaved     =False,\n#                       isEmbeddingMatrixFileSaved    = False,\n#                       wantToSaveEmbeedingIndexFile  = False,\n#                       wantToSaveEmbeedingMatrixFile = False\n                    \n                    \n                    \n# r = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 30):\n#         #For training use this member function","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:18:00.372001Z","iopub.execute_input":"2022-04-11T14:18:00.372506Z","iopub.status.idle":"2022-04-11T14:18:00.382449Z","shell.execute_reply.started":"2022-04-11T14:18:00.372440Z","shell.execute_reply":"2022-04-11T14:18:00.380244Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%time\nInappContentModel1Preprocessor = Preprocessor( embed_size = 300, \n                                               max_features = 130000, \n                                               max_len = 220, \n                                               model_Type = \"Inappropriate-Content-Detection\")\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:18:00.385408Z","iopub.execute_input":"2022-04-11T14:18:00.386416Z","iopub.status.idle":"2022-04-11T14:18:00.395463Z","shell.execute_reply.started":"2022-04-11T14:18:00.386341Z","shell.execute_reply":"2022-04-11T14:18:00.394285Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\n#HYPER PARAMETER 1\nvalidation_size = 0.3\nInappContentModel1Preprocessor.preprocessing( validation_size=validation_size,\n                                  embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n                                  embedding_path_glove =\"../input/glove840b300dtxt/glove.840B.300d.txt\" ,\n                                  isEmbeddingIndexFileSaved     = False,\n                                  isEmbeddingMatrixFileSaved    = False,\n                                  wantToSaveEmbeedingIndexFile  = False,\n                                  wantToSaveEmbeedingMatrixFile = False )","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:18:00.398926Z","iopub.execute_input":"2022-04-11T14:18:00.399597Z","iopub.status.idle":"2022-04-11T14:25:31.494648Z","shell.execute_reply.started":"2022-04-11T14:18:00.399563Z","shell.execute_reply":"2022-04-11T14:25:31.492662Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%time\nInappContentModel1 = ML_Model( InappContentModel1Preprocessor,\n                               file_path = \"1\",\n                               n_multiClassificationClasses = 6 )\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:25:31.496643Z","iopub.execute_input":"2022-04-11T14:25:31.496991Z","iopub.status.idle":"2022-04-11T14:25:31.505275Z","shell.execute_reply.started":"2022-04-11T14:25:31.496946Z","shell.execute_reply":"2022-04-11T14:25:31.504205Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#HYPER PARAMETER 2\nlearning_weight_decay = 0.1\n#HYPER PARAMETER 3\ndropout = 0.2\n#HYPER PARAMETER 4\nepochs = 50\nInappContentModel1.train_Model(lr = 1e-3, lr_d = learning_weight_decay, units = 112, dr = dropout, epochs = epochs )\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import tensorflow as tf\ntf.keras.utils.plot_model(InappContentModel1.model, to_file=f'Plotted_model-{InappContentModel1.modelBase.model_Type}--Framework-{InappContentModel1.framework_idea}-.png')\nInappContentModel1.model.summary()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:10:44.256318Z","iopub.execute_input":"2022-04-10T19:10:44.256571Z","iopub.status.idle":"2022-04-10T19:10:46.011402Z","shell.execute_reply.started":"2022-04-10T19:10:44.256534Z","shell.execute_reply":"2022-04-10T19:10:46.010484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(InappContentModel1.n_output_nuerons )\nprint(InappContentModel1.modelBase.embedding_matrix_glove.shape[0])\nprint(InappContentModel1.modelBase.max_features)","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:10:46.013135Z","iopub.execute_input":"2022-04-10T19:10:46.013713Z","iopub.status.idle":"2022-04-10T19:10:46.019424Z","shell.execute_reply.started":"2022-04-10T19:10:46.013672Z","shell.execute_reply":"2022-04-10T19:10:46.01872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred_train = InappContentModel1.get_accuracy_for_training_set()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:10:46.020624Z","iopub.execute_input":"2022-04-10T19:10:46.020875Z","iopub.status.idle":"2022-04-10T19:13:10.283021Z","shell.execute_reply.started":"2022-04-10T19:10:46.020841Z","shell.execute_reply":"2022-04-10T19:13:10.282239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred_valid  = InappContentModel1.get_accuracy_for_validation_set()","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:13:10.284523Z","iopub.execute_input":"2022-04-10T19:13:10.284944Z","iopub.status.idle":"2022-04-10T19:14:34.372277Z","shell.execute_reply.started":"2022-04-10T19:13:10.284905Z","shell.execute_reply":"2022-04-10T19:14:34.371457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappContentModel1.Plot( \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:14:34.373678Z","iopub.execute_input":"2022-04-10T19:14:34.373952Z","iopub.status.idle":"2022-04-10T19:14:34.70125Z","shell.execute_reply.started":"2022-04-10T19:14:34.373916Z","shell.execute_reply":"2022-04-10T19:14:34.700135Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappContentModel1.Plot( \"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-10T19:14:34.703565Z","iopub.execute_input":"2022-04-10T19:14:34.704043Z","iopub.status.idle":"2022-04-10T19:14:34.972162Z","shell.execute_reply.started":"2022-04-10T19:14:34.704014Z","shell.execute_reply":"2022-04-10T19:14:34.971443Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**#IDEA 2 IS BELOW:**","metadata":{}},{"cell_type":"code","source":"%%time\n# CHECKING FOR IDEA 2 BY SWITCHING \nInappContentModel1.change_idea_for_same_object( change_idea_to = \"2\", automatic_train = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:25:31.508857Z","iopub.execute_input":"2022-04-11T14:25:31.509595Z","iopub.status.idle":"2022-04-11T14:25:31.519384Z","shell.execute_reply.started":"2022-04-11T14:25:31.509535Z","shell.execute_reply":"2022-04-11T14:25:31.518116Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"#HYPER PARAMETER 2\nlearning_weight_decay = 0.1\n#HYPER PARAMETER 3\ndropout = 0.2\n#HYPER PARAMETER 4\nepochs = 50\nInappContentModel1.train_Model(lr = 1e-3, lr_d = learning_weight_decay, units = 112, dr = dropout, epochs = 5+epochs )\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:25:31.521352Z","iopub.execute_input":"2022-04-11T14:25:31.521922Z","iopub.status.idle":"2022-04-11T18:19:16.017490Z","shell.execute_reply.started":"2022-04-11T14:25:31.521875Z","shell.execute_reply":"2022-04-11T18:19:16.016418Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2 \ny_pred_valid  = InappContentModel1.get_accuracy_for_validation_set()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:19:16.019092Z","iopub.execute_input":"2022-04-11T18:19:16.020081Z","iopub.status.idle":"2022-04-11T18:20:07.600559Z","shell.execute_reply.started":"2022-04-11T18:19:16.020034Z","shell.execute_reply":"2022-04-11T18:20:07.599417Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2 \ny_pred_train = InappContentModel1.get_accuracy_for_training_set()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:20:08.281592Z","iopub.execute_input":"2022-04-11T18:20:08.282264Z","iopub.status.idle":"2022-04-11T18:22:32.844860Z","shell.execute_reply.started":"2022-04-11T18:20:08.282221Z","shell.execute_reply":"2022-04-11T18:22:32.842527Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA2\nInappContentModel1.Plot( \"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:20:07.602062Z","iopub.execute_input":"2022-04-11T18:20:07.602451Z","iopub.status.idle":"2022-04-11T18:20:07.986248Z","shell.execute_reply.started":"2022-04-11T18:20:07.602409Z","shell.execute_reply":"2022-04-11T18:20:07.984999Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA 2 \nInappContentModel1.Plot( \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:20:07.987855Z","iopub.execute_input":"2022-04-11T18:20:07.988819Z","iopub.status.idle":"2022-04-11T18:20:08.279822Z","shell.execute_reply.started":"2022-04-11T18:20:07.988769Z","shell.execute_reply":"2022-04-11T18:20:08.278676Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"#To download THE model:\nfrom IPython.display import FileLink  \nfilepath = f'./best_model_FrameWork2_Inappropriate-Content-Detection.hdf5'\nFileLink(filepath)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T18:28:58.579804Z","iopub.execute_input":"2022-04-11T18:28:58.580140Z","iopub.status.idle":"2022-04-11T18:28:58.588031Z","shell.execute_reply.started":"2022-04-11T18:28:58.580108Z","shell.execute_reply":"2022-04-11T18:28:58.586958Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SPAM DETECTION**","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n#HYPER PARAMETER 1\nvalidation_size = 0.3\n#HYPER PARAMETER 2\nlearning_weight_decay = 0.1\n#HYPER PARAMETER 3\ndropout = 0.2\n#HYPER PARAMETER 4\nepochs = 50\nspamDetectionModel1Preprocessor = Preprocessor( embed_size = 300, \n                                               max_features = 130000, \n                                               max_len = 220, \n                                               model_Type = \"Spam-Detection\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:29:48.598185Z","iopub.execute_input":"2022-04-11T13:29:48.598560Z","iopub.status.idle":"2022-04-11T13:29:48.604569Z","shell.execute_reply.started":"2022-04-11T13:29:48.598523Z","shell.execute_reply":"2022-04-11T13:29:48.603898Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"%%time\nspamDetectionModel1Preprocessor.preprocessing(  validation_size=validation_size,\n                                  embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n                                  embedding_path_glove =\"../input/glove840b300dtxt/glove.840B.300d.txt\" ,\n                                  isEmbeddingIndexFileSaved     = False,\n                                  isEmbeddingMatrixFileSaved    = False,\n                                  wantToSaveEmbeedingIndexFile  = False,\n                                  wantToSaveEmbeedingMatrixFile = False )","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:29:51.083492Z","iopub.execute_input":"2022-04-11T13:29:51.084043Z","iopub.status.idle":"2022-04-11T13:35:33.908559Z","shell.execute_reply.started":"2022-04-11T13:29:51.084004Z","shell.execute_reply":"2022-04-11T13:35:33.907750Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"%%time\nspamDetectionModel1 = ML_Model( spamDetectionModel1Preprocessor,\n                               file_path = \"1\",\n                               n_multiClassificationClasses = 1 )","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:35:33.910304Z","iopub.execute_input":"2022-04-11T13:35:33.910735Z","iopub.status.idle":"2022-04-11T13:35:33.917770Z","shell.execute_reply.started":"2022-04-11T13:35:33.910693Z","shell.execute_reply":"2022-04-11T13:35:33.917086Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"%%time\nspamDetectionModel1.train_Model(lr = 1e-3, lr_d = learning_weight_decay, units = 112, dr = dropout, epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:35:33.918894Z","iopub.execute_input":"2022-04-11T13:35:33.919211Z","iopub.status.idle":"2022-04-11T13:47:12.737370Z","shell.execute_reply.started":"2022-04-11T13:35:33.919175Z","shell.execute_reply":"2022-04-11T13:47:12.736634Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred_valid  = spamDetectionModel1.get_accuracy_for_validation_set()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:47:12.739493Z","iopub.execute_input":"2022-04-11T13:47:12.739752Z","iopub.status.idle":"2022-04-11T13:47:19.650713Z","shell.execute_reply.started":"2022-04-11T13:47:12.739717Z","shell.execute_reply":"2022-04-11T13:47:19.649173Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred_valid  = spamDetectionModel1.get_accuracy_for_training_set()","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:47:33.431720Z","iopub.execute_input":"2022-04-11T13:47:33.431977Z","iopub.status.idle":"2022-04-11T13:47:43.704337Z","shell.execute_reply.started":"2022-04-11T13:47:33.431949Z","shell.execute_reply":"2022-04-11T13:47:43.703473Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"spamDetectionModel1.Plot( \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:47:19.652005Z","iopub.execute_input":"2022-04-11T13:47:19.652357Z","iopub.status.idle":"2022-04-11T13:47:19.918712Z","shell.execute_reply.started":"2022-04-11T13:47:19.652318Z","shell.execute_reply":"2022-04-11T13:47:19.918013Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"spamDetectionModel1.Plot( \"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:47:19.920134Z","iopub.execute_input":"2022-04-11T13:47:19.920631Z","iopub.status.idle":"2022-04-11T13:47:20.173145Z","shell.execute_reply.started":"2022-04-11T13:47:19.920592Z","shell.execute_reply":"2022-04-11T13:47:20.172471Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#To download THE model:\nfrom IPython.display import FileLink  \nfilepath = f'./best_model_FrameWork1_Spam-Detection.hdf5'\nFileLink(filepath)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:49:19.893397Z","iopub.execute_input":"2022-04-11T13:49:19.893706Z","iopub.status.idle":"2022-04-11T13:49:19.900435Z","shell.execute_reply.started":"2022-04-11T13:49:19.893673Z","shell.execute_reply":"2022-04-11T13:49:19.899616Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"######################################################","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# CHECKING FOR IDEA 2 BY SWITCHING \nspamDetectionModel1.change_idea_for_same_object( change_idea_to = \"2\", automatic_train = False)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:49:54.333717Z","iopub.execute_input":"2022-04-11T13:49:54.334509Z","iopub.status.idle":"2022-04-11T13:49:54.340190Z","shell.execute_reply.started":"2022-04-11T13:49:54.334465Z","shell.execute_reply":"2022-04-11T13:49:54.339485Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"%%time\nspamDetectionModel1.train_Model(lr = 1e-3, lr_d = learning_weight_decay, units = 112, dr = dropout, epochs = epochs)","metadata":{"execution":{"iopub.status.busy":"2022-04-11T13:50:15.292274Z","iopub.execute_input":"2022-04-11T13:50:15.292851Z","iopub.status.idle":"2022-04-11T14:01:16.810233Z","shell.execute_reply.started":"2022-04-11T13:50:15.292812Z","shell.execute_reply":"2022-04-11T14:01:16.809488Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2\ny_pred_train = spamDetectionModel1.get_accuracy_for_training_set()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:01:16.812009Z","iopub.execute_input":"2022-04-11T14:01:16.812456Z","iopub.status.idle":"2022-04-11T14:01:25.649819Z","shell.execute_reply.started":"2022-04-11T14:01:16.812402Z","shell.execute_reply":"2022-04-11T14:01:25.649095Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2 \ny_pred_valid  = spamDetectionModel1.get_accuracy_for_validation_set()\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:01:25.651136Z","iopub.execute_input":"2022-04-11T14:01:25.651388Z","iopub.status.idle":"2022-04-11T14:01:30.798941Z","shell.execute_reply.started":"2022-04-11T14:01:25.651355Z","shell.execute_reply":"2022-04-11T14:01:30.798105Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA 2 \nspamDetectionModel1.Plot( \"loss\")\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:01:30.800743Z","iopub.execute_input":"2022-04-11T14:01:30.801087Z","iopub.status.idle":"2022-04-11T14:01:31.053745Z","shell.execute_reply.started":"2022-04-11T14:01:30.801047Z","shell.execute_reply":"2022-04-11T14:01:31.052599Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA 2\nspamDetectionModel1.Plot( \"accuracy\")\n","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:01:31.055062Z","iopub.execute_input":"2022-04-11T14:01:31.055447Z","iopub.status.idle":"2022-04-11T14:01:31.322019Z","shell.execute_reply.started":"2022-04-11T14:01:31.055391Z","shell.execute_reply":"2022-04-11T14:01:31.321243Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2\ny_pred_train = InappContentModel1.get_accuracy_for_training_set()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#To download THE model:\nfrom IPython.display import FileLink  \nFileLink(r'./best_model_FrameWork2_Spam-Detection.hdf5')","metadata":{"execution":{"iopub.status.busy":"2022-04-11T14:13:00.598918Z","iopub.execute_input":"2022-04-11T14:13:00.599196Z","iopub.status.idle":"2022-04-11T14:13:00.605847Z","shell.execute_reply.started":"2022-04-11T14:13:00.599165Z","shell.execute_reply":"2022-04-11T14:13:00.605082Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**PREDICTION**","metadata":{}},{"cell_type":"code","source":"#Importing required libraries \nimport pickle, time\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd, matplotlib.pyplot as plt \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\nfrom keras.models import Model, load_model\nimport logging\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nclass PredictorObject:\n    def __init__(self, max_len=220, \n                     max_features = 130000,\n                     model_Type = \"Spam-Detection\",\n                     file_path = \"1\",\n                    n_multiClassificationClasses = 1,# if Inappropriate then 6 as they are : \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"\n                    tweet_column = None,\n                 actual_model_path= None,\n                 training_dataset_path = \"../input/jigsaw-toxic-comment-classification-challenge/train.csv\",\n                     *args, **kwargs):\n        self.start_time = time.time()        \n        self.max_features = max_features\n        self.max_len = max_len\n        self.n_multiClassificationClasses  = n_multiClassificationClasses         \n        self.model_instanc_type = model_Type \n        self.framework_idea = file_path \n        \n        self.file_path = actual_model_path if actual_model_path else \"best_model_\" + \"FrameWork\" + self.framework_idea + \"_\" + self.model_instanc_type  + \".hdf5\"\n        self.result_filename = \"RESULTS.csv\"\n        \n        #check whether model is already prsent else load the model\n        self.model = load_model(self.file_path)\n        \n        self.tweet = None\n        self.tweet_array = None\n        self.tweet_df = None\n        self.original_tweet_df = None\n        self.raw_tweet_df = None\n        self.tweet_column,  self.list_classes = (\"Email-Text\", [\"ham-or-spam\"] ) if self.model_instanc_type == \"Spam-Detection\" else (\"Tweet-Comment\" , [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] )  \n        if tweet_column :\n            self.tweet_column = tweet_column\n        self.result = pd.DataFrame(columns = [self.tweet_column ] + self.list_classes) \n        \n        self.tk = Tokenizer(num_words = self.max_features, lower = True)\n        train = pd.read_csv(training_dataset_path)\n        if model_Type != \"Spam-Detection\":\n            train[\"comment_text\"].fillna(\"no comment\")\n            X_train, X_valid, Y_train, Y_valid = train_test_split( train, train[self.list_classes].values, test_size = 0.3, random_state=512)\n            ##Keeping hyperparaeter test_size as 0.3 \n            raw_text_train = X_train[\"comment_text\"].str.lower()\n            self.tk.fit_on_texts(raw_text_train)\n        else:\n            pass\n\n\n    def initializeAll(self, actual_model_path=None,  tweet_column=None):\n        self.tweet = None\n        self.tweet_array = None\n        self.tweet_df = None\n        self.raw_tweet_df = None\n        self.original_tweet_df = None\n        self.tweet_column,  self.list_classes = (\"Email-Text\", [\"ham-or-spam\"] ) if self.model_instanc_type == \"Spam-Detection\" else (\"Tweet-Comment\" , [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] )            \n        if tweet_column :\n            self.tweet_column = tweet_column\n        self.result = pd.DataFrame(columns = [self.tweet_column ] + self.list_classes) \n        self.result_filename = \"RESULTS.csv\"\n        self.file_path = actual_model_path if actual_model_path else \"best_model_\" + \"FrameWork\" + self.framework_idea + \"_\" + self.model_instanc_type  + \".hdf5\"\n        if actual_model_path:\n            self.model = load_model(self.file_path)\n            \n        \n        \n    def reLoadModel(self):\n        #check whether model is already prsent else load the model\n        self.model = load_model(self.file_path)\n        return\n        \n    def prediction(self, tweet_data=None, isText=True, isArray=False, isDataFrame=False, path_to_Tweet_DataFrame=None, result_filename=None, actual_model_path=None,  tweet_column=None ):\n        def exor(a, b): return ( a&(~b) or b&(~a) )        \n        if ( not exor( exor(isText, isArray), isDataFrame ) ) or ( isText == isArray and isText==isDataFrame ) :\n            print('404: all three or any two can not be true only one paramenters among isTweet, isDataFrame, isArray will be only true' )\n            return\n        self.initializeAll(actual_model_path,  tweet_column)\n        if isText:\n            self.tweet = tweet_data\n            self.tweet_array = [ tweet_data]\n            y_pred = self.preprocessDataFrame()\n        elif isArray:\n            self.tweet = None\n            self.tweet_array = tweet_data\n            y_pred = self.preprocessDataFrame()\n        else:\n            if path_to_Tweet_DataFrame :\n                self.tweet_df =  pd.read_csv(path_to_Tweet_DataFrame)\n                y_pred = self.preprocessDataFrame( override=True )\n            else:\n                print('ERROR : 404 FILE NOT FOUND!!! Provide valid path name.')\n                return\n        if not result_filename:\n            self.result_filename = result_filename\n        self.result[self.tweet_column] = self.original_tweet_df[self.tweet_column]\n#         self.result[self.tweet_column] = self.tweet_df.iloc[:,0]\n        self.result[self.list_classes] =  (y_pred)\n        self.result.to_csv(self.result_filename, index = False )\n        print(f\"[{ time.time() - self.start_time }] Completed!\")\n        return (self.result, self.tweet_column, self.list_classes, self.result_filename) \n\n    \n    def preprocessDataFrame(self,override=False):\n        if not override:\n            print('Inside')\n            self.tweet_df = pd.DataFrame( self.tweet_array, columns = [ self.tweet_column ])\n#         print(f\"\\n 1 , {self.tweet_df}\")\n        self.original_tweet_df = self.tweet_df.copy(deep=True)\n        self.tweet_df[self.tweet_column].fillna(\"no comment\")\n#         print(f\"\\n\\n 2 , {self.tweet_df}\")\n        self.raw_tweet_df = self.tweet_df[self.tweet_column].str.lower()\n#         print(f\"\\n\\n 3 , {self.tweet_df}\")\n#         self.tk.fit_on_texts(self.raw_tweet_df)\n        self.tweet_df[ self.tweet_column ] = self.tk.texts_to_sequences( self.raw_tweet_df )\n#         print(f\"\\n\\n 4 , {self.tweet_df}\")\n        self.tweet_df = pad_sequences( self.tweet_df[ self.tweet_column ], maxlen = self.max_len )\n#         print(f\"\\n\\n 5 , {self.tweet_df}\")\n        print(\"Predicting\")\n        y_pred = self.model.predict(self.tweet_df, verbose=0)\n        return y_pred","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2022-04-06T19:54:25.485731Z","iopub.execute_input":"2022-04-06T19:54:25.486183Z","iopub.status.idle":"2022-04-06T19:54:25.513362Z","shell.execute_reply.started":"2022-04-06T19:54:25.486146Z","shell.execute_reply":"2022-04-06T19:54:25.512523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inappropriate Content Detection Prediction**","metadata":{}},{"cell_type":"code","source":"%%time\ndf = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")\ntweet = df[\"comment_text\"][0]\n\n# actual_model_path_for_inapp = '../input/model-for-spam-inappropriate-content-detection1/best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\n# actual_model_path_for_inapp = './best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\nactual_model_path_for_inapp = '../input/inappcontentepoch50model/best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\nInappPrediction = PredictorObject(model_Type = \"Inappropriate-Content-Detection\", \n                                  file_path = \"1\" ,n_multiClassificationClasses = 6, \n                                  tweet_column='comment_text'  ,\n                                 actual_model_path = actual_model_path_for_inapp)\n\n\n\ntest_data_path = '../input/jigsaw-toxic-comment-classification-challenge/test.csv'\nresult, tweet_column, list_classes, result_filename = InappPrediction.prediction( tweet,isText=True, isArray=False, isDataFrame=False, tweet_column='comment_text'  )","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:54:29.859151Z","iopub.execute_input":"2022-04-06T19:54:29.859748Z","iopub.status.idle":"2022-04-06T19:54:54.773882Z","shell.execute_reply.started":"2022-04-06T19:54:29.859708Z","shell.execute_reply":"2022-04-06T19:54:54.773129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.tweet_array","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:54:54.775424Z","iopub.execute_input":"2022-04-06T19:54:54.775697Z","iopub.status.idle":"2022-04-06T19:54:54.783585Z","shell.execute_reply.started":"2022-04-06T19:54:54.77566Z","shell.execute_reply":"2022-04-06T19:54:54.782802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.raw_tweet_df[0] ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:54:54.785257Z","iopub.execute_input":"2022-04-06T19:54:54.785945Z","iopub.status.idle":"2022-04-06T19:54:54.793377Z","shell.execute_reply.started":"2022-04-06T19:54:54.785872Z","shell.execute_reply":"2022-04-06T19:54:54.792509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.raw_tweet_df","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:54:54.795678Z","iopub.execute_input":"2022-04-06T19:54:54.796004Z","iopub.status.idle":"2022-04-06T19:54:54.804132Z","shell.execute_reply.started":"2022-04-06T19:54:54.795968Z","shell.execute_reply":"2022-04-06T19:54:54.803345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.result.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:54:54.806999Z","iopub.execute_input":"2022-04-06T19:54:54.807217Z","iopub.status.idle":"2022-04-06T19:54:54.826032Z","shell.execute_reply.started":"2022-04-06T19:54:54.807193Z","shell.execute_reply":"2022-04-06T19:54:54.825349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ntweet ='you will fucking bitch cry whole life and i will cut you in pieces. Beacause i am monster. Be ready, i am coming for you and when i will find you , i will wash my hand with your blood'\nresult, tweet_column, list_classes, result_filename = InappPrediction.prediction( tweet,isText=True, isArray=False, isDataFrame=False, tweet_column='comment_text'  )\nprint(\"\\n\\n\",InappPrediction.tweet_array,\"\\n\\n\")\nInappPrediction.result.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:54:54.827176Z","iopub.execute_input":"2022-04-06T19:54:54.827503Z","iopub.status.idle":"2022-04-06T19:54:54.908919Z","shell.execute_reply.started":"2022-04-06T19:54:54.827466Z","shell.execute_reply":"2022-04-06T19:54:54.908195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.result.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:55:15.380695Z","iopub.execute_input":"2022-04-06T19:55:15.381244Z","iopub.status.idle":"2022-04-06T19:55:15.393577Z","shell.execute_reply.started":"2022-04-06T19:55:15.381206Z","shell.execute_reply":"2022-04-06T19:55:15.392666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(result)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:55:36.434225Z","iopub.execute_input":"2022-04-06T19:55:36.434516Z","iopub.status.idle":"2022-04-06T19:55:36.44002Z","shell.execute_reply.started":"2022-04-06T19:55:36.43448Z","shell.execute_reply":"2022-04-06T19:55:36.439275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:57:05.071684Z","iopub.execute_input":"2022-04-06T19:57:05.071957Z","iopub.status.idle":"2022-04-06T19:57:05.083646Z","shell.execute_reply.started":"2022-04-06T19:57:05.071926Z","shell.execute_reply":"2022-04-06T19:57:05.082866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result[list_classes[-1]][0]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:57:43.426619Z","iopub.execute_input":"2022-04-06T19:57:43.42713Z","iopub.status.idle":"2022-04-06T19:57:43.43322Z","shell.execute_reply.started":"2022-04-06T19:57:43.427094Z","shell.execute_reply":"2022-04-06T19:57:43.432486Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_classes","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:58:52.294873Z","iopub.execute_input":"2022-04-06T19:58:52.295145Z","iopub.status.idle":"2022-04-06T19:58:52.300389Z","shell.execute_reply.started":"2022-04-06T19:58:52.295114Z","shell.execute_reply":"2022-04-06T19:58:52.299351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for classes in list_classes:\n    print(classes,result[classes][0])","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:59:39.413919Z","iopub.execute_input":"2022-04-06T19:59:39.414245Z","iopub.status.idle":"2022-04-06T19:59:39.42089Z","shell.execute_reply.started":"2022-04-06T19:59:39.414214Z","shell.execute_reply":"2022-04-06T19:59:39.419949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result[tweet_column][0]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:58:17.496193Z","iopub.execute_input":"2022-04-06T19:58:17.4965Z","iopub.status.idle":"2022-04-06T19:58:17.503857Z","shell.execute_reply.started":"2022-04-06T19:58:17.496446Z","shell.execute_reply":"2022-04-06T19:58:17.502989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = (255,8,98)","metadata":{"execution":{"iopub.status.busy":"2022-04-06T20:12:33.810016Z","iopub.execute_input":"2022-04-06T20:12:33.810689Z","iopub.status.idle":"2022-04-06T20:12:33.813972Z","shell.execute_reply.started":"2022-04-06T20:12:33.81065Z","shell.execute_reply":"2022-04-06T20:12:33.813294Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f[3]","metadata":{"execution":{"iopub.status.busy":"2022-04-06T20:12:41.976337Z","iopub.execute_input":"2022-04-06T20:12:41.977029Z","iopub.status.idle":"2022-04-06T20:12:41.997785Z","shell.execute_reply.started":"2022-04-06T20:12:41.976993Z","shell.execute_reply":"2022-04-06T20:12:41.996558Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tweet_column, list_classes, result_filename ","metadata":{"execution":{"iopub.status.busy":"2022-04-06T19:55:51.267646Z","iopub.execute_input":"2022-04-06T19:55:51.268207Z","iopub.status.idle":"2022-04-06T19:55:51.274245Z","shell.execute_reply.started":"2022-04-06T19:55:51.268168Z","shell.execute_reply":"2022-04-06T19:55:51.273189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# actual_model_path_for_inapp = '../input/model-for-spam-inappropriate-content-detection1/best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\nactual_model_path_for_inapp = './best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\nInappPrediction = PredictorObject(model_Type = \"Inappropriate-Content-Detection\", \n                                  file_path = \"1\" ,n_multiClassificationClasses = 6, \n                                  tweet_column='comment_text'  ,\n                                 actual_model_path = actual_model_path_for_inapp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Checking whether model is loaded correctly or not \nfrom sklearn.metrics import roc_auc_score\ndef predictions(dataset, y_actual,model,on_the_fly=0 ): # On the fly is for those data that is comimg on the fly from any tweet.\n    y_pred_validation = model.predict(dataset, verbose=1)\n    score_validation = roc_auc_score(y_actual, y_pred_validation)\n    print(f\"\\n ROC-AUC - ON given Dataset - score: {round(score_validation,5)*100}%\")\n    return y_pred_validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################################\npredictions(InappContentModel1.modelBase.X_train,InappContentModel1.modelBase.Y_train,InappPrediction.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########################################################################\npredictions(InappContentModel1.modelBase.X_valid,InappContentModel1.modelBase.Y_valid,InappPrediction.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# test_data_path = '../input/jigsaw-toxic-comment-classification-challenge/test.csv'\n# InappPrediction.prediction( None,isText=False, isArray=False, isDataFrame=True, path_to_Tweet_DataFrame=test_data_path, tweet_column='comment_text'  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n# train_data_path = '../input/jigsaw-toxic-comment-classification-challenge/train.csv'\ndf1 = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf1[\"comment_text\"].fillna(\"no comment\")\nX_train, X_valid, Y_train, Y_valid = train_test_split(df1, df1[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values, test_size = 0.1)\n\n# raw_text_train = X_train[\"comment_text\"].str.lower()\ndf2 = X_train['comment_text'].tolist()\ndf3 =X_valid['comment_text'].tolist()\n# for i in range(len(X_train['comment_text']))\n#     df2.append(X_train['comment_text'][i])\n# for i in range(len(X_valid['comment_text'])):\n#     df3.append(X_valid['comment_text'][i])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappContentModel1.modelBase.X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train['comment_text'][0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# df3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(X_train['comment_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# X_train['comment_text'][2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(df2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df1['comment_text'][2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df2[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for item in df2[:2]:\n#     print(item)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result2, tweet_column2, list_classes2, result_filename2   = InappPrediction.prediction( df2,isText=False, isArray=True, isDataFrame=False, path_to_Tweet_DataFrame=None,tweet_column='comment_text'  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.result.head(12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].head(12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.result[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].shape == InappPrediction.result[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\ndef predictions(dataset, y_actual,model,on_the_fly=0 ): # On the fly is for those data that is comimg on the fly from any tweet.\n    y_pred_validation = model.predict(dataset,  batch_size = 1024, verbose = 1)\n    score_validation = roc_auc_score(y_actual, y_pred_validation)\n    print(f\"\\n ROC-AUC - ON given Dataset - score: {round(score_validation,5)*100}%\")\n    return y_pred_validation","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# predictions( InappPrediction.tweet_df, Y_train, InappPrediction.model)\npredictions( InappPrediction.tweet_df, Y_train, InappContentModel1.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions( InappContentModel1.modelBase.X_train,InappContentModel1.modelBase.Y_train, InappContentModel1.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.tweet_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappContentModel1.modelBase.X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.original_tweet_df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"result3, tweet_column3, list_classes3, result_filename3   = InappPrediction.prediction( df3,isText=False, isArray=True, isDataFrame=False, path_to_Tweet_DataFrame=None,tweet_column='comment_text'  )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappPrediction.result.head(12)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions( InappPrediction.tweet_df, Y_valid, InappPrediction.model)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Spam  Detection Prediction**","metadata":{}},{"cell_type":"code","source":"%%time\ndf = pd.read_csv(\"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")\ntweet = df[df.columns[0]][0]\n\n# actual_model_path_for_inapp = '../input/model-for-spam-inappropriate-content-detection1/best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\n# actual_model_path_for_inapp = './best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\nactual_model_path_for_inapp = '../input/inappcontentepoch50model/best_model_FrameWork1_Inappropriate-Content-Detection.hdf5'\nInappPrediction = PredictorObject(model_Type = \"Inappropriate-Content-Detection\", \n                                  file_path = \"1\" ,n_multiClassificationClasses = 6, \n                                  tweet_column='comment_text'  ,\n                                 actual_model_path = actual_model_path_for_inapp)\n\n\n\ntest_data_path = '../input/jigsaw-toxic-comment-classification-challenge/test.csv'\nresult, tweet_column, list_classes, result_filename = InappPrediction.prediction( tweet,isText=True, isArray=False, isDataFrame=False, tweet_column='comment_text'  )","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# len(df1['comment_text'])","metadata":{"execution":{"iopub.status.busy":"2022-03-13T18:43:14.165317Z","iopub.execute_input":"2022-03-13T18:43:14.166147Z","iopub.status.idle":"2022-03-13T18:43:14.175383Z","shell.execute_reply.started":"2022-03-13T18:43:14.166098Z","shell.execute_reply":"2022-03-13T18:43:14.173899Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class PredictorObject:\n#     def __init(self, max_len=220, \n#                      max_features = 130000,\n#                      model_Type = \"Spam-Detection\",\n#                      file_path = \"1\",\n#                     n_multiClassificationClasses = 1,# if Inappropriate then 6 as they are : \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"\n#                     *args, **kwargs):\n#         self.start_time = time.time        \n#         self.max_features = max_features\n#         self.max_len = max_len\n#         self.n_multiClassificationClasses  = n_multiClassificationClasses         \n#         self.model_instanc_type = model_Type \n#         self.framework_idea = file_path \n#         self.tk = Tokenizer(num_words = self.max_features, lower = True)\n#         self.file_path = \"best_model_\" + \"FrameWork\" + self.framework_idea + \"_\" + self.model_instanc_type  + \".hdf5\"\n        \n#         #check whether model is already prsent else load the model\n#         self.model = load_model(self.file_path)\n        \n#         self.tweet = None\n#         self.tweet_array = None\n#         self.tweet_df = None\n#         self.raw_tweet_df = None\n#         self.preprocessor_module = None\n#         self.tweet_columns,  self.list_classes = (\"Email-Text\", [\"ham-or-spam\"] ) if self.model_instanc_type == \"Spam-Detection\" else (\"Tweet-Comment\" , [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"] )            \n\n#     def initializeAll(self):\n#         self.tweet = None\n#         self.tweet_array = None\n#         self.tweet_df = None\n#         self.raw_tweet_df = None\n#         self.tweet_columns = \"Tweet-Comment\" if self.model_instanc_type != \"Spam-Detection\" else \"Email-Text\" \n#         self.preprocessor_module = None\n        \n#     def prediction(self, tweet_data, isText=True, isArray=False, isDataFrame=False ):\n#         def exor(a, b): return ( a&(~b) or b&(~a)           \n#         if ( not exor( exor(isText, isArray), isDataFrame ) )or ( isText == isArray and isText==isDataFrame ) :\n#             reurn '404: all three or any two can not be true only one paramenters among isTweet, isDataFrame, isArray will be only true' \n#         if isText:\n#             self.tweet = tweet_data\n#             self.tweet_array = list(tweet_data)\n#             y_pred = self.preprocessDataFram()\n#         elif isArray:\n#         else:\n#         self.submission[list_classes] = (y_pred)\n#         submission.to_csv(\"submission.csv\", index = False)\n#         print(\"[{}] Completed!\".format(time.time() - self.start_time))\n            \n# #         submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n#         self.tweet_array or self.tweet = None #'given tweet array or convert it into an array'\n#         pass\n    \n#     def preprocessDataFrame(self):\n#         self.tweet_df = pd.DataFrame( self.tweet_array, columns = [ self.tweet_columns ])\n#         self.tweet_df[self.tweet_column].fillna(\"no comment\")\n#         self.raw_tweet_df = self.tweet_df[self.tweet_column].str.lower()\n#         self.tweet_df[ self.tweet_column ] = self.tk.texts_to_sequences( self.raw_tweet_df )\n#         self.tweet_df = pad_sequences( self.tweet_df[ self.tweet_column ], maxlen = self.max_len ) \n#         y_pred = self.model.predict(self.tweet_df, verbose=0)\n#         return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-03-12T09:26:02.719541Z","iopub.execute_input":"2022-03-12T09:26:02.721156Z","iopub.status.idle":"2022-03-12T09:26:02.733123Z","shell.execute_reply.started":"2022-03-12T09:26:02.721105Z","shell.execute_reply":"2022-03-12T09:26:02.731642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv')\n\ndf_test = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\n","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:02:25.156764Z","iopub.execute_input":"2022-03-12T16:02:25.157031Z","iopub.status.idle":"2022-03-12T16:02:26.896456Z","shell.execute_reply.started":"2022-03-12T16:02:25.157001Z","shell.execute_reply":"2022-03-12T16:02:26.895559Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.head(10)","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:02:27.912462Z","iopub.execute_input":"2022-03-12T16:02:27.91302Z","iopub.status.idle":"2022-03-12T16:02:27.938852Z","shell.execute_reply.started":"2022-03-12T16:02:27.912982Z","shell.execute_reply":"2022-03-12T16:02:27.938057Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test","metadata":{"execution":{"iopub.status.busy":"2022-03-12T16:02:34.868468Z","iopub.execute_input":"2022-03-12T16:02:34.869068Z","iopub.status.idle":"2022-03-12T16:02:34.880975Z","shell.execute_reply.started":"2022-03-12T16:02:34.869028Z","shell.execute_reply":"2022-03-12T16:02:34.880166Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **IDEA -1 BI-GRU & LSTM WITH DUAL EMBEDDINGS(FAST TEXT + GLOVE).**","metadata":{}},{"cell_type":"code","source":"#Importing all the libraries \n\nimport time\nstart_time = time.time()\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\n# from keras.engine import InputSpec, Layer\nfrom keras.layers import Layer\nfrom keras.layers import InputSpec\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport_time = round((time.time()-start_time)*1000,3)\n\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nprint(f\"'ALL LIBRARIES IMPORTED SUCCESSFULLY!!' in TIME : {import_time} msec\")\n","metadata":{"execution":{"iopub.status.busy":"2022-02-26T17:06:24.287486Z","iopub.execute_input":"2022-02-26T17:06:24.28767Z","iopub.status.idle":"2022-02-26T17:06:24.297811Z","shell.execute_reply.started":"2022-02-26T17:06:24.287646Z","shell.execute_reply":"2022-02-26T17:06:24.297066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Customized Class to handle ROC AUC Evaluation for each epoch and printing epoch number and score for each epoch which is multiple of interval.  \nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n            \n            ","metadata":{"execution":{"iopub.status.busy":"2022-02-26T17:06:24.299055Z","iopub.execute_input":"2022-02-26T17:06:24.299642Z","iopub.status.idle":"2022-02-26T17:06:24.309212Z","shell.execute_reply.started":"2022-02-26T17:06:24.299609Z","shell.execute_reply":"2022-02-26T17:06:24.307646Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the training and testing data i.e converting that into dataframes\ntrain = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest  = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-26T17:06:24.310745Z","iopub.execute_input":"2022-02-26T17:06:24.311042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Columns of training data\ntrain.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Columns of test data\ntest.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nembedding_path_glove    = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\nembed_size = 300\nmax_features = 130000 #Vocabulary Size \nmax_len = 220  # maximum length of tweet. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\ntrain[\"comment_text\"].fillna(\"no comment\")\ntest[\"comment_text\"].fillna(\"no comment\")\n\n\nvalidation_size = 0.1 \n# splitting train:dev -->  9:1 so train consist of 90 percent and validation set consist of 10 percent of entire training data \n#Note : This validation_size is configurable and can be changed later.\nX_train, X_valid, Y_train, Y_valid = train_test_split(train, y, test_size = validation_size)\n\n\n\n# Lowering all the comments of training, validation and test data let callled it as raw.\nraw_text_train = X_train[\"comment_text\"].str.lower()\nraw_text_valid = X_valid[\"comment_text\"].str.lower()\nraw_text_test = test[\"comment_text\"].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Artcle for better undrstanding of Tokenizer : https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/\n\n# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\ntk = Tokenizer(num_words = max_features)\n#Tokeinzing the raw training set\ntk.fit_on_texts(raw_text_train)\n# print(raw_text_train.shape )\nX_train[\"comment_seq\"] = tk.texts_to_sequences(raw_text_train)\nX_valid[\"comment_seq\"] = tk.texts_to_sequences(raw_text_valid)\ntest[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test)\n\nX_train = pad_sequences(X_train.comment_seq, maxlen = max_len)\nX_valid = pad_sequences(X_valid.comment_seq, maxlen = max_len)\ntest = pad_sequences(test.comment_seq, maxlen = max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Embeeding Index which can help further to create embedding matrix for the words in our training dataset vocabulary.\n# This Ebedding index is created from the fastText or Glove.\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\n#Embedding Index correpsonding to FastText\nembedding_index_fastText = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_fastText))\n\n#Embedding Index correpsonding to Glove\nembedding_index_glove = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_glove))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(embedding_index_fastText))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(embedding_index_glove))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embedding_index_fastText.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embedding_index_glove.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embedding_index_fastText['spam'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embedding_index_glove['spam'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index_fastText['spam'][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index_glove['spam'][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index_fastText['spam'][-10+1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_index_glove['spam'][-10+1:]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing Our Embeeding matrix from Embedding Index from glove or fastText. \ndamword_index = tk.word_index\nnb_words = min(max_features, len(damword_index))\nembedding_matrix_fastText = np.zeros((nb_words, embed_size))\nembedding_matrix_glove = np.zeros((nb_words, embed_size))\n\nfor word, i in damword_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index_fastText.get(word)\n    if embedding_vector is not None: embedding_matrix_fastText[i] = embedding_vector\n        \nfor word, i in damword_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index_glove.get(word)\n    if embedding_vector is not None: embedding_matrix_glove[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_fastText[2][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_glove[2][:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embedding_matrix_fastText[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(embedding_matrix_glove[2])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam, RMSprop\n# from keras.optimizers import  RMSprop, adam_v2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = \"best_model_idea1.hdf5\"  #Nmae by whihc Model is to be saved \ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n\n#Allowing early stopping\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n    inp = Input(shape = (max_len,))\n    x_fastText = Embedding(max_features, embed_size, weights = [embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n    x_glove = Embedding(max_features, embed_size, weights = [embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n    \n    #Drop-Out Layer\n    x1_fastText = SpatialDropout1D(dr)(x_fastText)\n    x1_glove = SpatialDropout1D(dr)(x_glove)\n\n    ### FastText\n    #BI-GRU , FastText\n    x_gru_fastText = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n    x_conv_gru_fastText = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_gru_fastText)\n    \n    #BI-LSTM , FastText\n    x_lstm_fastText = Bidirectional(LSTM(units, return_sequences = True))(x1_fastText)\n    x_conv_lstm_fastText = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_lstm_fastText)\n    \n    ### Glove\n    #BI-GRU , Glove\n    x_gru_glove = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n    x_conv_gru_glove = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_gru_glove)\n    \n    #BI-LSTM , Glove\n    x_lstm_glove = Bidirectional(LSTM(units, return_sequences = True))(x1_glove)\n    x_conv_lstm_glove = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_lstm_glove)\n    \n    \n    ### FastText\n    #POOLING_CONV ,BI-GRU, FastText\n    avg_pool1_gru_fastText = GlobalAveragePooling1D()(x_conv_gru_fastText)\n    max_pool1_gru_fastText = GlobalMaxPooling1D()(x_conv_gru_fastText)\n    \n    #POOLING_CONV ,BI-LSTM, FastText\n    avg_pool2_lstm_fastText = GlobalAveragePooling1D()(x_conv_lstm_fastText)\n    max_pool2_lstm_fastText = GlobalMaxPooling1D()(x_conv_lstm_fastText)\n    \n    \n    ### Glove\n    #POOLING_CONV ,BI-GRU, Glove\n    avg_pool1_gru_glove = GlobalAveragePooling1D()(x_conv_gru_glove)\n    max_pool1_gru_glove = GlobalMaxPooling1D()(x_conv_gru_glove)\n    \n    #POOLING_CONV ,BI-LSTM, Glove\n    avg_pool2_lstm_glove = GlobalAveragePooling1D()(x_conv_lstm_glove)\n    max_pool2_lstm_glove = GlobalMaxPooling1D()(x_conv_lstm_glove)\n    \n    \n    #Concatenating FastText Branch\n    x_fastText = concatenate([avg_pool1_gru_fastText, max_pool1_gru_fastText, avg_pool2_lstm_fastText, max_pool2_lstm_fastText])\n    \n    #Concatenating Glove Branch\n    x_glove = concatenate([avg_pool1_gru_glove, max_pool1_gru_glove, avg_pool2_lstm_glove, max_pool2_lstm_glove])\n  \n    \n#     x = x_fastText  #Comment this line when below line got  un-commented\n    #Finally Concatenating Both the branches\n    x = concatenate([x_fastText, x_glove])\n    \n    #Passing concatenated output to dense network\n    x = Dense(6, activation = \"sigmoid\")(x)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n#     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, Y_train, batch_size = 128, epochs = epochs, validation_data = (X_valid, Y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model,history\n    \n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trainng The Model\nmodel,history = build_model(lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 3)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting The Model\n\npred = model.predict(test, batch_size = 1024, verbose = 1)\n\nsubmission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nsubmission[list_classes] = (pred)\nsubmission.to_csv(\"submission.csv\", index = False)\nprint(\"[{}] Completed!\".format(time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test[0].shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred[132]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROC-AUC Evaluation  \n# https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n\n\ny_pred_validation = model.predict(X_valid, verbose=1)\nscore_validation = roc_auc_score(Y_valid, y_pred_validation)\nprint(f\"\\n ROC-AUC - ON Validation Dataset - score: {round(score_validation,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = model.predict(X_train, verbose=1)\nscore_train = roc_auc_score(Y_train, y_pred_train)\nprint(f\"\\n ROC-AUC - Training Dataset - score: {round(score_train,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_valid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred = model.predict(X_valid,verbose=1)\n# pred_prob = model.predict_proba(X_valid)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_valid[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pred)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(pred[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\ndef Plot(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_' + string])\n    plt.xlabel(\"EPOCHS\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_' + string ])\n    plt.savefig(string + '_IDEA1.png')\n    plt.show()\n    \n    \nPlot(history, \"accuracy\")\nPlot(history, \"loss\")\n\n\nprint(\"###### DONE PLOTTING FOR IDEA 1 ########\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **ROC-AUC PLOTTING**","metadata":{}},{"cell_type":"code","source":"predict_probability = np.zeros(shape=pred.shape,dtype='int64')\nno_classes = len(pred[0])\nno_of_examples = len(pred)\nfor i in range(no_of_examples):\n    maxi_index = 0\n    maximum_element = pred[i][0]\n    for j in range(1,no_classes):\n        if maximum_element<pred[i][j]:\n            maximum_element = pred[i][j]\n            maxi_index = j\n    predict_probability[i][maxi_index] = 1\n      \n            \n        \n    \n        \n        ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_probability","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_probability.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_valid.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_valid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# roc_pred = np.zeros(no_of_examples,dtype='int64')\n# for i in range(no_of_examples):\n#     for j in range(no_classes):\n#         if pred[i][j]==1:\n#             roc_pred[i]=j\n#             break\n            \n            \ny_validation = np.zeros(no_of_examples,dtype='int64')\nfor i in range(no_of_examples):\n    for j in range(no_classes):\n        if Y_valid[i][j]==1:\n            y_validation[i]=j\n            break","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# roc curve for classes\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfpr = {}\ntpr = {}\nthresh ={}\n\nn_class = no_classes\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_validation, predict_probability[:,i], pos_label=i)\n    \n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plotting    \nplt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\nplt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\nplt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\nplt.plot(fpr[3], tpr[3], linestyle='--',color='red', label='Class 3 vs Rest')\nplt.plot(fpr[4], tpr[4], linestyle='--',color='black', label='Class 4 vs Rest')\nplt.plot(fpr[5], tpr[5], linestyle='--',color='brown', label='Class 5 vs Rest')\nplt.title('Multiclass ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300); ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #  multi-class classification\n# from sklearn.multiclass import OneVsRestClassifier\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import roc_curve\n# from sklearn.metrics import roc_auc_score\n# from sklearn.datasets import make_classification\n# from sklearn.model_selection import train_test_split\n# import matplotlib.pyplot as plt\n\n# # generate 2 class dataset\n# X, y = make_classification(n_samples=1000, n_classes=3, n_features=20, n_informative=3, random_state=42)\n\n# # split into train/test sets\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n\n# # fit model\n# clf = OneVsRestClassifier(LogisticRegression())\n# clf.fit(X_train, y_train)\n# pred = clf.predict(X_test)\n# pred_prob = clf.predict_proba(X_test)\n\n# # roc curve for classes\n# fpr = {}\n# tpr = {}\n# thresh ={}\n\n# n_class = 3\n\n\n\n# for i in range(n_class):    \n#     fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n    \n# # plotting    \n# plt.plot(fpr[0], tpr[0], linestyle='--',color='orange', label='Class 0 vs Rest')\n# plt.plot(fpr[1], tpr[1], linestyle='--',color='green', label='Class 1 vs Rest')\n# plt.plot(fpr[2], tpr[2], linestyle='--',color='blue', label='Class 2 vs Rest')\n# plt.title('Multiclass ROC curve')\n# plt.xlabel('False Positive Rate')\n# plt.ylabel('True Positive rate')\n# plt.legend(loc='best')\n# plt.savefig('Multiclass ROC',dpi=300);    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pred_prob","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_fastText.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_glove.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#  **DUAL EMBEDDINGS**","metadata":{}},{"cell_type":"code","source":"# import numpy as np\n# import pandas as pd\n# import os\n# from keras.models import Model\n# from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n# from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, CuDNNGRU, Conv1D\n# from keras.preprocessing import text, sequence\n# from keras.callbacks import LearningRateScheduler\n# from sklearn.metrics import roc_auc_score\n# from sklearn.model_selection import train_test_split\n# import tensorflow as tf\n# print(tf.__version__)\n# tf.test.is_gpu_available(\n#     cuda_only=False,\n#     min_cuda_compute_capability=None\n# )","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# EMBEDDING_FILES = [\n#         '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n#     '../input/glove840b300dtxt/glove.840B.300d.txt'\n# ]\n\n\n# BATCH_SIZE = 512\n# LSTM_UNITS = 128\n# DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n# EPOCHS = 4\n# MAX_LEN = 220\n\n\n# TEXT_COLUMN = 'comment_text'\n# list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n# CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\n# test_df = pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\n# submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n\n# y = train_df[list_classes].values\n# x_train = train_df[TEXT_COLUMN].astype(str)\n# y_train = y\n# x_test = test_df[TEXT_COLUMN].astype(str)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_coefs(word, *arr):\n#     return word, np.asarray(arr, dtype='float32')\n\n\n# def load_embeddings(path):\n#     with open(path) as f:\n#         return dict(get_coefs(*line.strip().split(' ')) for line in f)\n\n\n# def build_matrix(word_index, path):\n#     embedding_index = load_embeddings(path)\n#     embedding_matrix = np.zeros((len(word_index) + 1, 300))\n#     for word, i in word_index.items():\n#         try:\n#             embedding_matrix[i] = embedding_index[word]\n#         except KeyError:\n#             pass\n#     return embedding_matrix\n\n# def build_model(embedding_matrix):\n#     words = Input(shape=(None,))\n#     x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n#     #x = SpatialDropout1D(0.2)(x)\n\n#     x1 = SpatialDropout1D(0.2)(x)\n\n#     x = Bidirectional(CuDNNGRU(LSTM_UNITS, return_sequences = True))(x1)\n#     x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n    \n#     y = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences = True))(x1)\n#     y = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(y)\n\n\n#     avg_pool1 = GlobalAveragePooling1D()(x)\n#     max_pool1 = GlobalMaxPooling1D()(x)\n   \n#     avg_pool2 = GlobalAveragePooling1D()(y)\n#     max_pool2 = GlobalMaxPooling1D()(y)\n   \n\n#     x = concatenate([avg_pool1, max_pool1, avg_pool2, max_pool2])\n\n\n#     x = Dense(6, activation = \"sigmoid\")(x)\n\n#     model = Model(inputs = words, outputs = x)\n\n#     model.compile(loss = \"binary_crossentropy\", optimizer = \"adam\", metrics = [\"accuracy\"])\n\n#     return model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\n# tokenizer.fit_on_texts(list(x_train) + list(x_test))\n\n# x_train = tokenizer.texts_to_sequences(x_train)\n# x_test = tokenizer.texts_to_sequences(x_test)\n# x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n# x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# %%time\n# embedding_matrix = np.concatenate(\n#     [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **IDEA -2 BI-GRU & LSTM WITH DUAL EMBEDDINGS(FAST TEXT + GLOVE).**","metadata":{}},{"cell_type":"code","source":"#Importing all the libraries \n\nprint(\"WORKING ON IDEA 2....\")\n\nimport time\nstart_time = time.time()\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\n# from keras.engine import InputSpec, Layer\nfrom keras.layers import Layer\nfrom keras.layers import InputSpec\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport_time = round((time.time()-start_time)*1000,3)\n\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nprint(f\"'ALL LIBRARIES IMPORTED SUCCESSFULLY!!' in TIME : {import_time} msec\")\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Customized Class to handle ROC AUC Evaluation for each epoch and printing epoch number and score for each epoch which is multiple of interval.  \nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the training and testing data i.e converting that into dataframes\ntrain = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/train.csv\")\ntest  = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/test.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Columns of training data\ntrain.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Columns of test data\ntest.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nembedding_path_glove    = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\nembed_size = 300\nmax_features = 130000 #Vocabulary Size \nmax_len = 220  # maximum length of tweet. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[list_classes].values\ntrain[\"comment_text\"].fillna(\"no comment\")\ntest[\"comment_text\"].fillna(\"no comment\")\n\nvalidation_size = 0.1 \n# splitting train:dev -->  9:1 so train consist of 90 percent and validation set consist of 10 percent of entire training data \n#Note : This validation_size is configurable and can be changed later.\nX_train, X_valid, Y_train, Y_valid = train_test_split(train, y, test_size = validation_size)\n\n\n\n# Lowering all the comments of training, validation and test data let callled it as raw.\nraw_text_train = X_train[\"comment_text\"].str.lower()\nraw_text_valid = X_valid[\"comment_text\"].str.lower()\nraw_text_test = test[\"comment_text\"].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Article for better undrstanding of Tokenizer : https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/\n\n# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\ntk = Tokenizer(num_words = max_features)\n#Tokeinzing the raw training set\ntk.fit_on_texts(raw_text_train)\n# print(raw_text_train.shape )\nX_train[\"comment_seq\"] = tk.texts_to_sequences(raw_text_train)\nX_valid[\"comment_seq\"] = tk.texts_to_sequences(raw_text_valid)\ntest[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test)\n\nX_train = pad_sequences(X_train.comment_seq, maxlen = max_len)\nX_valid = pad_sequences(X_valid.comment_seq, maxlen = max_len)\ntest = pad_sequences(test.comment_seq, maxlen = max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Embeeding Index which can help further to create embedding matrix for the words in our training dataset vocabulary.\n# This Ebedding index is created from the fastText or Glove.\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\n#Embedding Index correpsonding to FastText\nembedding_index_fastText = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_fastText))\n\n#Embedding Index correpsonding to Glove\nembedding_index_glove = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_glove))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing Our Embeeding matrix from Embedding Index from glove or fastText. \ndamword_index = tk.word_index\nnb_words = min(max_features, len(damword_index))\nembedding_matrix_fastText = np.zeros((nb_words, embed_size))\nembedding_matrix_glove = np.zeros((nb_words, embed_size))\n\nfor word, i in damword_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index_fastText.get(word)\n    if embedding_vector is not None: embedding_matrix_fastText[i] = embedding_vector\n        \nfor word, i in damword_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index_glove.get(word)\n    if embedding_vector is not None: embedding_matrix_glove[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam, RMSprop\n# from keras.optimizers import  RMSprop, adam_v2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = \"best_model_idea2.hdf5\"  #Nmae by whihc Model is to be saved \ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n\n#Allowing early stopping\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n    inp = Input(shape = (max_len,))\n    x_fastText = Embedding(max_features, embed_size, weights = [embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n    x_glove = Embedding(max_features, embed_size, weights = [embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n    \n    #Drop-Out Layer\n    x1_fastText = SpatialDropout1D(dr)(x_fastText)\n    x1_glove = SpatialDropout1D(dr)(x_glove)\n    \n    \n    #BI-LSTM BRANCH\n    x_glove_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n    x_fastText_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n    \n    \n    #BI-GRU BRNACH\n    x_glove_gru = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n    x_fastText_gru = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n    \n    \n    #Convolutional+Pooling Layer This is Optional Can be commnented later if required for testing purposes\n    \n    #Bi-listm\n    x_glove_lstm_conv           = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_lstm)\n    x_glove_lstm_conv_avgPool   = GlobalAveragePooling1D()(x_glove_lstm_conv)\n    x_glove_lstm_conv_maxPool   = GlobalMaxPooling1D()(x_glove_lstm_conv)\n    #concatenating\n    x_glove_lstm_conv_pooled    = concatenate([x_glove_lstm_conv_avgPool, x_glove_lstm_conv_maxPool])\n    \n    \n    \n    x_fastText_lstm_conv        = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_lstm)\n    x_fastText_lstm_conv_avgPool= GlobalAveragePooling1D()(x_fastText_lstm_conv)\n    x_fastText_lstm_conv_maxPool= GlobalMaxPooling1D()(x_fastText_lstm_conv)\n    #concatenating\n    x_fastText_lstm_conv_pooled = concatenate([x_fastText_lstm_conv_avgPool, x_fastText_lstm_conv_maxPool])\n    \n    \n    #Bi-gru\n    x_glove_gru_conv            = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_gru)\n    x_glove_gru_conv_avgPool    = GlobalAveragePooling1D()(x_glove_gru_conv)\n    x_glove_gru_conv_maxPool    = GlobalMaxPooling1D()(x_glove_gru_conv)\n    #concatenating\n    x_glove_gru_conv_pooled     = concatenate([x_glove_gru_conv_avgPool, x_glove_gru_conv_maxPool])\n    \n    \n    \n    x_fastText_gru_conv         = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_gru)\n    x_fastText_gru_conv_avgPool = GlobalAveragePooling1D()(x_fastText_gru_conv)\n    x_fastText_gru_conv_maxPool = GlobalMaxPooling1D()(x_fastText_gru_conv)\n    #concatenating\n    x_fastText_gru_conv_pooled  = concatenate([x_fastText_gru_conv_avgPool, x_fastText_gru_conv_maxPool])\n    \n    \n    \n    #Con-Catenating Bi-Lstm branch\n    x_lstm_conv_pooled = concatenate([x_glove_lstm_conv_pooled,  x_fastText_lstm_conv_pooled])\n    \n    #Con-Catenating Bi-Gru branch\n    x_gru_conv_pooled = concatenate([x_glove_gru_conv_pooled,  x_fastText_gru_conv_pooled])\n    \n    #ConCatenating Bi-LSTM and Bi-GRU Branch\n    x_lstm_gru_concatenated = concatenate([x_lstm_conv_pooled, x_gru_conv_pooled])\n    \n    \n    #Passing concatenated output to dense network\n    x = Dense(6, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n#     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, Y_train, batch_size = 128, epochs = epochs, validation_data = (X_valid, Y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model,history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trainng The Model\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#Training The Model\nmodel,history = build_model(lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 3)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Predicting The Model\n\npred = model.predict(test, batch_size = 1024, verbose = 1)\n\nsubmission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\nsubmission[list_classes] = (pred)\nsubmission.to_csv(\"submissionIdea2.csv\", index = False)\nprint(\"[{}] Completed!\".format(time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROC-AUC Evaluation  \n# https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n\n\ny_pred_validation = model.predict(X_valid, verbose=1)\nscore_validation = roc_auc_score(Y_valid, y_pred_validation)\nprint(f\"\\n ROC-AUC - ON Validation Dataset - score: {round(score_validation,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = model.predict(X_train, verbose=1)\nscore_train = roc_auc_score(Y_train, y_pred_train)\nprint(f\"\\n ROC-AUC - Training Dataset - score: {round(score_train,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef Plot(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_' + string])\n    plt.xlabel(\"EPOCHS\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_' + string ])\n    plt.savefig(string + '_IDEA2.png')\n    plt.show()\n    \n    \nPlot(history, \"accuracy\")\nPlot(history, \"loss\")\n\n\nprint(\"###### DONE PLOTTING FOR IDEA 2 ########\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **IDEA 1 FOR SPAM DETECTION**","metadata":{}},{"cell_type":"code","source":"#Importing all the libraries \n\nprint(\"WORKING ON IDEA 2....\")\n\nimport time\nstart_time = time.time()\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\n# from keras.engine import InputSpec, Layer\nfrom keras.layers import Layer\nfrom keras.layers import InputSpec\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport_time = round((time.time()-start_time)*1000,3)\n\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nprint(f\"'ALL LIBRARIES IMPORTED SUCCESSFULLY!!' in TIME : {import_time} msec\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Customized Class to handle ROC AUC Evaluation for each epoch and printing epoch number and score for each epoch which is multiple of interval.  \nclass RocAucEvaluation(Callback):\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Reading the training and testing data i.e converting that into dataframes\ntrain_df1 = pd.read_csv(\"../input/spam-or-not-spam-dataset/spam_or_not_spam.csv\")\ntrain_df2  = pd.read_csv(\"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.rename(columns={train_df1.columns[0] : \"Message\", train_df1.columns[1] : \"Category\" },inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2[train_df2.columns[0]].replace({\"ham\": 0, \"spam\": 1}, inplace=True)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2 = train_df2[[train_df2.columns[1],train_df2.columns[0]]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train_df1, train_df2], ignore_index=True, sort=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ny = train[\"Category\"].values\n# y = np.asarray(y).astype(np.float32)\ntrain[\"Message\"].fillna(\"no comment\")\n# train_df2[\"Message\"].fillna(\"no comment\")\n\nvalidation_size = 0.1 \n# splitting train:dev -->  9:1 so train consist of 90 percent and validation set consist of 10 percent of entire training data \n#Note : This validation_size is configurable and can be changed later.\nX_train, X_valid, Y_train, Y_valid = train_test_split(train, y, test_size = validation_size)\n\n\n\n# Lowering all the comments of training, validation and test data let callled it as raw.\nraw_text_train = X_train[\"Message\"].str.lower()\nraw_text_valid = X_valid[\"Message\"].str.lower()\n# raw_test_valid = X_test[\"Message\"].str.lower()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_text_train.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_text_train[raw_text_train.isnull()].index.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nembedding_path_glove    = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\nembed_size = 300\nmax_features = 130000 #Vocabulary Size \nmax_len = 220  # maximum length of tweet. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Article for better undrstanding of Tokenizer : https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/\n\n# num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\ntk = Tokenizer(num_words = max_features)\n\n#Tokeinzing the raw training set\ntk.fit_on_texts(raw_text_train)\nprint(raw_text_train.shape )\n\n\n\nX_train[\"Message\"] = tk.texts_to_sequences(raw_text_train)\nX_valid[\"Message\"] = tk.texts_to_sequences(raw_text_valid)\n# test[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test)\n\nX_train = pad_sequences(X_train.Message, maxlen = max_len)\nX_valid = pad_sequences(X_valid.Message, maxlen = max_len)\n# test = pad_sequences(test.comment_seq, maxlen = max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Creating Embeeding Index which can help further to create embedding matrix for the words in our training dataset vocabulary.\n# This Ebedding index is created from the fastText or Glove.\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n\n#Embedding Index correpsonding to FastText\nembedding_index_fastText = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_fastText))\n\n#Embedding Index correpsonding to Glove\nembedding_index_glove = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_glove))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(tk.word_index.keys())","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing Our Embeeding matrix from Embedding Index from glove or fastText. \ndamword_index = tk.word_index\nnb_words = min(max_features, 1+len(damword_index))\nembedding_matrix_fastText = np.zeros((nb_words, embed_size))\nembedding_matrix_glove = np.zeros((nb_words, embed_size))\n\nfor word, i in damword_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index_fastText.get(word)\n    if embedding_vector is not None: embedding_matrix_fastText[i] = embedding_vector\n        \nfor word, i in damword_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index_glove.get(word)\n    if embedding_vector is not None: embedding_matrix_glove[i] = embedding_vector","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam, RMSprop\n# from keras.optimizers import  RMSprop, adam_v2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = \"best_model_idea1_SPAM-DETECTION.hdf5\"  #Nmae by whihc Model is to be saved \ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n\n#Allowing early stopping\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n    inp = Input(shape = (max_len,))\n    x_fastText = Embedding(min(max_features,1+len(tk.word_index)), embed_size, weights = [embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n    x_glove = Embedding(min(max_features,1+len(tk.word_index)), embed_size, weights = [embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n    \n    #Drop-Out Layer\n    x1_fastText = SpatialDropout1D(dr)(x_fastText)\n    x1_glove = SpatialDropout1D(dr)(x_glove)\n    \n    \n    #BI-LSTM BRANCH\n    x_glove_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n    x_fastText_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n    \n    \n    #BI-GRU BRNACH\n    x_glove_gru = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n    x_fastText_gru = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n    \n    \n    #Convolutional+Pooling Layer This is Optional Can be commnented later if required for testing purposes\n    \n    #Bi-listm\n    x_glove_lstm_conv           = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_lstm)\n    x_glove_lstm_conv_avgPool   = GlobalAveragePooling1D()(x_glove_lstm_conv)\n    x_glove_lstm_conv_maxPool   = GlobalMaxPooling1D()(x_glove_lstm_conv)\n    #concatenating\n    x_glove_lstm_conv_pooled    = concatenate([x_glove_lstm_conv_avgPool, x_glove_lstm_conv_maxPool])\n    \n    \n    \n    x_fastText_lstm_conv        = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_lstm)\n    x_fastText_lstm_conv_avgPool= GlobalAveragePooling1D()(x_fastText_lstm_conv)\n    x_fastText_lstm_conv_maxPool= GlobalMaxPooling1D()(x_fastText_lstm_conv)\n    #concatenating\n    x_fastText_lstm_conv_pooled = concatenate([x_fastText_lstm_conv_avgPool, x_fastText_lstm_conv_maxPool])\n    \n    \n    #Bi-gru\n    x_glove_gru_conv            = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_gru)\n    x_glove_gru_conv_avgPool    = GlobalAveragePooling1D()(x_glove_gru_conv)\n    x_glove_gru_conv_maxPool    = GlobalMaxPooling1D()(x_glove_gru_conv)\n    #concatenating\n    x_glove_gru_conv_pooled     = concatenate([x_glove_gru_conv_avgPool, x_glove_gru_conv_maxPool])\n    \n    \n    \n    x_fastText_gru_conv         = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_gru)\n    x_fastText_gru_conv_avgPool = GlobalAveragePooling1D()(x_fastText_gru_conv)\n    x_fastText_gru_conv_maxPool = GlobalMaxPooling1D()(x_fastText_gru_conv)\n    #concatenating\n    x_fastText_gru_conv_pooled  = concatenate([x_fastText_gru_conv_avgPool, x_fastText_gru_conv_maxPool])\n    \n    \n    \n    #Con-Catenating Bi-Lstm branch\n    x_lstm_conv_pooled = concatenate([x_glove_lstm_conv_pooled,  x_fastText_lstm_conv_pooled])\n    \n    #Con-Catenating Bi-Gru branch\n    x_gru_conv_pooled = concatenate([x_glove_gru_conv_pooled,  x_fastText_gru_conv_pooled])\n    \n    #ConCatenating Bi-LSTM and Bi-GRU Branch\n    x_lstm_gru_concatenated = concatenate([x_lstm_conv_pooled, x_gru_conv_pooled])\n    \n    \n    #Passing concatenated output to dense network\n#     xxxx = Dense(8, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n#     xxx = Dense(4, activation = \"sigmoid\")(xxxx)\n#     xx = Dense(2, activation = \"sigmoid\")(xx)\n#     x = Dense(1, activation = \"sigmoid\")(xx)\n\n    \n    x = Dense(1, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n#     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, Y_train, batch_size = 128, epochs = epochs, validation_data = (X_valid, Y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model,history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trainng The Model\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#Training The Model\nmodel,history = build_model(lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 30)\n\n#best is 7","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Predicting The Model\n\n# pred = model.predict(test, batch_size = 1024, verbose = 1)\n\n# submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n# submission[list_classes] = (pred)\n# submission.to_csv(\"submissionIdea2.csv\", index = False)\n# print(\"[{}] Completed!\".format(time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROC-AUC Evaluation  \n# https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n\n\ny_pred_validation = model.predict(X_valid, verbose=1)\nscore_validation = roc_auc_score(Y_valid, y_pred_validation)\nprint(f\"\\n ROC-AUC - ON Validation Dataset - score: {round(score_validation,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = model.predict(X_train, verbose=1)\nscore_train = roc_auc_score(Y_train, y_pred_train)\nprint(f\"\\n ROC-AUC - Training Dataset - score: {round(score_train,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\ndef Plot(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_' + string])\n    plt.xlabel(\"EPOCHS\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_' + string ])\n    plt.savefig(string + '_IDEA1.png')\n    plt.show()\n    \n    \nPlot(history, \"accuracy\")\nPlot(history, \"loss\")\n\n\nprint(\"###### DONE PLOTTING FOR IDEA 1 ########\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# ****IDEA 2 FOR SPAM DETECTION****","metadata":{}},{"cell_type":"markdown","source":"**Importing all the libraries**","metadata":{}},{"cell_type":"code","source":"#Importing all the libraries \n\nprint(\"WORKING ON IDEA 2....\")\n\nimport time\nstart_time = time.time()\n\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd, matplotlib.pyplot as plt \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras.layers import InputSpec\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n# from keras.optimizers import  RMSprop, adam_v2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport_time = round((time.time()-start_time)*1000,3)\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\nprint(f\"'ALL LIBRARIES IMPORTED SUCCESSFULLY!!' in TIME : {import_time} msec\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Epoch Class Handler**","metadata":{}},{"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    '''\n     Customized Class to handle ROC AUC Evaluation for each epoch and printing epoch number and score for each epoch which is multiple of interval.  \n    '''\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Reading the dataset**","metadata":{}},{"cell_type":"code","source":"def read_dataset(filepath):\n    '''\n    Reading the training and testing data i.e converting that into dataframes\n\n    input : file path \n    ----------------\n    output : dataframe \n    '''\n    df = pd.read_csv(filepath)\n    return df ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1 = read_dataset(\"../input/spam-or-not-spam-dataset/spam_or_not_spam.csv\")\ntrain_df2  = read_dataset(\"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_df1 = pd.read_csv(\"../input/spam-or-not-spam-dataset/spam_or_not_spam.csv\")\n# train_df2  = pd.read_csv(\"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.rename(columns={train_df1.columns[0] : \"Message\", train_df1.columns[1] : \"Category\" },inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.columns[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2[train_df2.columns[0]].replace({\"ham\": 0, \"spam\": 1}, inplace=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2 = train_df2[[train_df2.columns[1],train_df2.columns[0]]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Constant and Embeeding PATH**","metadata":{}},{"cell_type":"code","source":"embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\"\nembedding_path_glove    = \"../input/glove840b300dtxt/glove.840B.300d.txt\"\nembed_size = 300\nmax_features = 130000 #Vocabulary Size \nmax_len = 220  # maximum length of tweet. ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = pd.concat([train_df1, train_df2], ignore_index=True, sort=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.tail(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df1.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df2.tail(2)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.dropna()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_test_builder(train,validation_size=0.1):\n    '''\n    Input : training dataset Dataframe -> train\n    ------------\n    Output : Training and dev dataframe with X and Y values along with raw train and dev set with all comments in lower.\n    '''\n    # list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n    y = train[\"Category\"].values\n    # y = np.asarray(y).astype(np.float32)\n    train[\"Message\"].fillna(\"no comment\")\n    # train_df2[\"Message\"].fillna(\"no comment\")\n\n    validation_size = validation_size   #by default it is 0.1\n    # splitting train:dev -->  9:1 so train consist of 90 percent and validation set consist of 10 percent of entire training data \n    #Note : This validation_size is configurable and can be changed later.\n    X_train, X_valid, Y_train, Y_valid = train_test_split(train, y, test_size = validation_size)\n\n    # Lowering all the comments of training, validation and test data let callled it as raw.\n    raw_text_train = X_train[\"Message\"].str.lower()\n    raw_text_valid = X_valid[\"Message\"].str.lower()\n    # raw_test_valid = X_test[\"Message\"].str.lower()\n    \n    \n#     print(y.dtype)\n\n    return (X_train,X_valid, Y_train, Y_valid, raw_text_train, raw_text_valid, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_valid, Y_train, Y_valid, raw_text_train, raw_text_valid, y = train_test_builder(train, validation_size=0.1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_text_train.isnull().sum().sum()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_text_train[raw_text_train.isnull()].index.tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Tokinizing**","metadata":{}},{"cell_type":"code","source":"def tokenizer(raw_text_train, raw_text_valid, X_train, X_valid, max_features, max_len):\n    \n    '''\n    Input : raw train and dev set along with in-rawed train and dev set.\n    --------------------------------------------------------------------------------\n    Output: tokenizer object along with padded and sequenced training and validation dataset\n    '''\n    \n    # Article for better undrstanding of Tokenizer : https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/\n\n    # num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n    tk = Tokenizer(num_words = max_features)\n\n    #Tokeinzing the raw training set\n    tk.fit_on_texts(raw_text_train)\n    print(raw_text_train.shape )\n    \n    X_train[\"Message\"] = tk.texts_to_sequences(raw_text_train)\n    X_valid[\"Message\"] = tk.texts_to_sequences(raw_text_valid)\n    # test[\"comment_seq\"] = tk.texts_to_sequences(raw_text_test)\n\n    X_train = pad_sequences(X_train.Message, maxlen = max_len)\n    X_valid = pad_sequences(X_valid.Message, maxlen = max_len)\n    # test = pad_sequences(test.comment_seq, maxlen = max_len)\n    \n    return (tk, X_train, X_valid)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tk, X_train, X_valid = tokenizer(raw_text_train, raw_text_valid, X_train, X_valid, max_features, max_len)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Embedding Index Builer**","metadata":{}},{"cell_type":"code","source":"\ndef get_coefs(word,*arr): \n    '''\n    Creating Embeeding Index which can help further to create embedding matrix for the words in our training dataset vocabulary.\n    This Ebedding index is created from the fastText or Glove.\n    '''\n    return word, np.asarray(arr, dtype='float32')\n\n\n\n\n#Embedding Index correpsonding to FastText\nembedding_index_fastText = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_fastText))\n\n#Embedding Index correpsonding to Glove\nembedding_index_glove = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path_glove))\n\nprint(len(tk.word_index.keys()))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Embdding Matrix Builder**","metadata":{}},{"cell_type":"code","source":"def embeeding_Matrix_Builder(tk, max_features, embed_size, embedding_index_fastText, embedding_matrix_glove):\n    '''\n    Input : tokenizer object , with maximum features and embedding size along with fastText and Glove Embedding Index for building Embeeding Matrix\n    ----------------------------------------------------------------------------------------------------------------------------------------------\n    Output: embedding matix corresponding to FastText and Glove\n    '''\n    # Preparing Our Embeeding matrix from Embedding Index from glove or fastText. \n    damword_index = tk.word_index\n    nb_words = min(max_features, len(damword_index))\n    embedding_matrix_fastText = np.zeros((nb_words+1, embed_size))\n    embedding_matrix_glove = np.zeros((nb_words+1, embed_size))\n\n    for word, i in damword_index.items():\n        if i >= max_features: continue\n        embedding_vector = embedding_index_fastText.get(word)\n        if embedding_vector is not None: embedding_matrix_fastText[i] = embedding_vector\n\n    for word, i in damword_index.items():\n        if i >= max_features: continue\n        embedding_vector = embedding_index_glove.get(word)\n        if embedding_vector is not None: embedding_matrix_glove[i] = embedding_vector\n            \n    return embedding_matrix_fastText, embedding_matrix_glove \n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"embedding_matrix_fastText, embedding_matrix_glove = embeeding_Matrix_Builder(tk, max_features, embed_size, embedding_index_fastText, embedding_matrix_glove)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = \"best_model_idea2_spamDETECTION.hdf5\"  #Nmae by whihc Model is to be saved \ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\nra_val = RocAucEvaluation(validation_data=(X_valid, Y_valid), interval = 1)\n\n#Allowing early stopping\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n    inp = Input(shape = (max_len,))\n    x_fastText = Embedding(min(max_features,1+len(tk.word_index)), embed_size, weights = [embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n    x_glove = Embedding(min(max_features,1+len(tk.word_index)), embed_size, weights = [embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n    \n    #Drop-Out Layer\n    x1_fastText = SpatialDropout1D(dr)(x_fastText)\n    x1_glove = SpatialDropout1D(dr)(x_glove)\n    \n    \n    #BI-LSTM BRANCH\n    x_glove_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n    x_fastText_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n    \n    \n    #BI-GRU BRNACH\n    x_glove_gru = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n    x_fastText_gru = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n    \n    \n    #Convolutional+Pooling Layer This is Optional Can be commnented later if required for testing purposes\n    \n    #Bi-listm\n    x_glove_lstm_conv           = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_lstm)\n    x_glove_lstm_conv_avgPool   = GlobalAveragePooling1D()(x_glove_lstm_conv)\n    x_glove_lstm_conv_maxPool   = GlobalMaxPooling1D()(x_glove_lstm_conv)\n    #concatenating\n    x_glove_lstm_conv_pooled    = concatenate([x_glove_lstm_conv_avgPool, x_glove_lstm_conv_maxPool])\n    \n    \n    \n    x_fastText_lstm_conv        = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_lstm)\n    x_fastText_lstm_conv_avgPool= GlobalAveragePooling1D()(x_fastText_lstm_conv)\n    x_fastText_lstm_conv_maxPool= GlobalMaxPooling1D()(x_fastText_lstm_conv)\n    #concatenating\n    x_fastText_lstm_conv_pooled = concatenate([x_fastText_lstm_conv_avgPool, x_fastText_lstm_conv_maxPool])\n    \n    \n    #Bi-gru\n    x_glove_gru_conv            = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_gru)\n    x_glove_gru_conv_avgPool    = GlobalAveragePooling1D()(x_glove_gru_conv)\n    x_glove_gru_conv_maxPool    = GlobalMaxPooling1D()(x_glove_gru_conv)\n    #concatenating\n    x_glove_gru_conv_pooled     = concatenate([x_glove_gru_conv_avgPool, x_glove_gru_conv_maxPool])\n    \n    \n    \n    x_fastText_gru_conv         = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_gru)\n    x_fastText_gru_conv_avgPool = GlobalAveragePooling1D()(x_fastText_gru_conv)\n    x_fastText_gru_conv_maxPool = GlobalMaxPooling1D()(x_fastText_gru_conv)\n    #concatenating\n    x_fastText_gru_conv_pooled  = concatenate([x_fastText_gru_conv_avgPool, x_fastText_gru_conv_maxPool])\n    \n    \n    \n    #Con-Catenating Bi-Lstm branch\n    x_lstm_conv_pooled = concatenate([x_glove_lstm_conv_pooled,  x_fastText_lstm_conv_pooled])\n    \n    #Con-Catenating Bi-Gru branch\n    x_gru_conv_pooled = concatenate([x_glove_gru_conv_pooled,  x_fastText_gru_conv_pooled])\n    \n    #ConCatenating Bi-LSTM and Bi-GRU Branch\n    x_lstm_gru_concatenated = concatenate([x_lstm_conv_pooled, x_gru_conv_pooled])\n    \n    \n    #Passing concatenated output to dense network\n#     xxxx = Dense(8, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n#     xxx = Dense(4, activation = \"sigmoid\")(xxxx)\n#     xx = Dense(2, activation = \"sigmoid\")(xxx)\n#     x = Dense(1, activation = \"sigmoid\")(xx)\n\n    x = Dense(1, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n#     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, Y_train, batch_size = 128, epochs = epochs, validation_data = (X_valid, Y_valid), \n                        verbose = 1, callbacks = [ra_val, check_point, early_stop])\n    model = load_model(file_path)\n    return model,history","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Trainng The Model\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#Training The Model\nmodel,history = build_model(lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 30)\n#best is 3 to 5","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# #Predicting The Model\n# pred = model.predict(t, batch_size = 1024, verbose = 1)\n\n# submission = pd.read_csv(\"../input/jigsaw-toxic-comment-classification-challenge/sample_submission.csv\")\n# submission[list_classes] = (pred)\n# submission.to_csv(\"submissionIdea2.csv\", index = False)\n# print(\"[{}] Completed!\".format(time.time() - start_time))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def prediction(dataset, y_actual):\n    y_pred_validation = model.predict(dataset, verbose=1)\n    score_validation = roc_auc_score(y_actual, y_pred_validation)\n    print(f\"\\n ROC-AUC - ON Validation Dataset - score: {round(score_validation,5)*100}%\")\n    return y_pred_validation\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_validation = prediction(X_valid, Y_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_train = prediction(X_train, Y_train)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#ROC-AUC Evaluation  \n# https://www.analyticsvidhya.com/blog/2020/06/auc-roc-curve-machine-learning/\n\n# y_pred_validation = model.predict(X_valid, verbose=1)\n# score_validation = roc_auc_score(Y_valid, y_pred_validation)\n# print(f\"\\n ROC-AUC - ON Validation Dataset - score: {round(score_validation,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred_train = model.predict(X_train, verbose=1)\n# score_train = roc_auc_score(Y_train, y_pred_train)\n# print(f\"\\n ROC-AUC - Training Dataset - score: {round(score_train,5)*100}%\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Plot(history, string):\n    plt.plot(history.history[string])\n    plt.plot(history.history['val_' + string])\n    plt.xlabel(\"EPOCHS\")\n    plt.ylabel(string)\n    plt.legend([string, 'val_' + string ])\n    plt.savefig(string + '_IDEA2.png')\n    plt.show()\n    \n    \nPlot(history, \"accuracy\")\nPlot(history, \"loss\")\nprint(\"###### DONE PLOTTING FOR IDEA 2 ########\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **BUILDING MODEL IN OOPS WAY FOR BOTH SPAM/INAPP CONTENT DETECTION**\nThus this helps in providing abstraction and easy way to deploy model as well as will also help in providing an machine learning API for our proposed work. ","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Importing all the libraries \n\nprint(\"WORKING ON ML Models in object oriented way....\")\n\nimport time\nstart_time = time.time()\n# from numpy import asarray\n# from numpy import loadtxt\n# from numpy import savetxt\nimport pickle\nfrom sklearn.model_selection import train_test_split\nimport sys, os, re, csv, codecs, numpy as np, pandas as pd, matplotlib.pyplot as plt \nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.layers import Layer\nfrom keras.layers import InputSpec\nimport logging\nfrom sklearn.metrics import roc_auc_score\nfrom keras.callbacks import Callback\nfrom tensorflow.keras.optimizers import Adam, RMSprop\n# from keras.optimizers import  RMSprop, adam_v2\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom keras.layers import GRU, BatchNormalization, Conv1D, MaxPooling1D\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport_time = round((time.time()-start_time)*1000,3)\nnp.random.seed(32)\nos.environ[\"OMP_NUM_THREADS\"] = \"4\"\n\n\nprint(f\"'\\n\\nALL LIBRARIES IMPORTED SUCCESSFULLY!!' in TIME : {import_time} msec\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:13:14.31872Z","iopub.execute_input":"2022-02-27T16:13:14.319038Z","iopub.status.idle":"2022-02-27T16:13:19.578598Z","shell.execute_reply.started":"2022-02-27T16:13:14.319002Z","shell.execute_reply":"2022-02-27T16:13:19.577802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RocAucEvaluation(Callback):\n    '''\n     Customized Class to handle ROC AUC Evaluation for each epoch and printing epoch number and score for each epoch which is multiple of interval.  \n    '''\n    def __init__(self, validation_data=(), interval=1):\n        super(Callback, self).__init__()\n\n        self.interval = interval\n        self.X_val, self.y_val = validation_data\n\n    def on_epoch_end(self, epoch, logs={}):\n        if epoch % self.interval == 0:\n            y_pred = self.model.predict(self.X_val, verbose=0)\n            score = roc_auc_score(self.y_val, y_pred)\n            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))\n\n#################################################################################################################################################\n\nclass Modelbase :\n    def __init__(self, embed_size = 300, max_features = 130000, max_len = 220, model_Type = \"Spam-Detection\", *args, **kwargs,):\n        self.model_Type = model_Type\n        self.embed_size = embed_size\n        self.max_features = max_features #Vocabulary Size \n        self.max_len = max_len  # maximum length of tweet. \n        self.X_train = None\n        self.Y_train = None\n        self.X_valid = None\n        self.Y_valid = None\n        self.train = None\n        self.y = None\n        self.raw_text_train = None \n        self.raw_text_valid= None\n        if self.model_Type == \"Spam-Detection\":\n            self.list_classes = [\"ham\", \"spam\"]\n            self.tweet_column = \"Message\"\n            self.train_df1 = None\n            self.train_df2 = None            \n        else:\n            self.list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n            self.tweet_column = \"comment_text\"\n            self.test = None\n            self.raw_test_valid= None        \n            self.X_test =  None\n            \n        \n    def read_dataset(self,filepath):\n        '''\n        Reading the training and testing data i.e converting that into dataframes\n        input : file path \n        ----------------\n        output : dataframe \n        '''\n        df = pd.read_csv(filepath)\n        return df \n    \n    def build_train_test_dev_set(self, \n                                 trainDF1 = \"../input/spam-or-not-spam-dataset/spam_or_not_spam.csv\" , \n                                 trainDF2 = \"../input/spam-text-message-classification/SPAM text message 20170820 - Data.csv\" , \n                                 trainDF  = \"../input/jigsaw-toxic-comment-classification-challenge/train.csv\" , \n                                 testDF   = \"../input/jigsaw-toxic-comment-classification-challenge/test.csv\"  ):\n        \n        if self.model_Type == \"Spam-Detection\"  :\n            self.train_df1 = self.read_dataset(trainDF1)\n            self.train_df2 = self.read_dataset(trainDF2)\n            self.spam_Dataset_Aggregator()\n            self.train = self.train.dropna()\n            return \n        else:    \n            self.train  = self.read_dataset(trainDF)\n            self.test  = self.read_dataset(testDF)\n#             self.train = self.train.dropna()\n            return\n        \n    def spam_Dataset_Aggregator(self):\n        self.train_df1.rename(columns={self.train_df1.columns[0] : \"Message\", self.train_df1.columns[1] : \"Category\" },inplace=True)\n        self.train_df2[self.train_df2.columns[0]].replace({\"ham\": 0, \"spam\": 1}, inplace=True)\n        self.train_df2 = self.train_df2[ [ self.train_df2.columns[1], self.train_df2.columns[0] ] ]\n        self.train = pd.concat([self.train_df1, self.train_df2], ignore_index=True, sort=False)\n        return  \n        \n    def __repr__(self):\n        if self.model_Type == \"Spam-Detection\":\n            return f\"Model for Spam Detection (Ham/Spam detection) \\n Model Type :  {self.model_Type}\"\n        return f\"Model for Inappropraue Content Detection (Ham/Spam detection) \\n Model Type :  {self.model_Type}\" #Inappropriate-Content-Detection\n\n    \n    \n#######################################################################################################################################################\n\n\n\nclass Preprocessor(Modelbase):\n    \n    def __init__(self,embed_size = 300, \n                 max_features = 130000, \n                 max_len = 220, \n                 model_Type = \"Spam-Detection\", \n                 *args, **kwargs):\n#         super(Model, self).__init__()\n        super().__init__(embed_size , max_features, max_len, model_Type)\n        # num_words = the maximum number of words to keep, based on word frequency. Only the most common num_words-1 words will be kept.\n        self.tk = Tokenizer(num_words = max_features)\n        self.embedding_matrix_fastText = None\n        self.embedding_matrix_glove = None\n        self.embedding_index_fastText = None\n        self.embedding_index_glove = None\n        self.embedding_matrix_fastText_shape = None\n        self.embedding_matrix_glove_shape = None\n#         self.embedding_index_fastText_shape = None\n#         self.embedding_index_glove_shape = None\n        \n    def train_test_splitter(self,validation_size=0.1):\n        '''\n        Input : training dataset Dataframe -> train\n        ------------\n        Output : Training and dev dataframe with X and Y values along with raw train and dev set with all comments in lower.\n        '''\n        if self.model_Type != \"Spam-Detection\" :\n            # y = np.asarray(y).astype(np.float32)\n            self.y = self.train[self.list_classes].values\n            self.train[self.tweet_column].fillna(\"no comment\")\n            self.test[self.tweet_column].fillna(\"no comment\") \n#             self.train = self.train.dropna()  #**********************************************************\n        else:\n            self.y = self.train[\"Category\"].values\n            # y = np.asarray(y).astype(np.float32)\n            self.train[self.tweet_column].fillna(\"no comment\")\n        # train_df2[\"Message\"].fillna(\"no comment\")\n        validation_size = validation_size   #by default it is 0.1\n        # splitting train:dev -->  9:1 so train consist of 90 percent and validation set consist of 10 percent of entire training data \n        #Note : This validation_size is configurable and can be changed later.\n        self.X_train, self.X_valid, self.Y_train, self.Y_valid = train_test_split(self.train, self.y, test_size = validation_size)\n        # Lowering all the comments of training, validation and test data let callled it as raw.\n        self.raw_text_train = self.X_train[self.tweet_column].str.lower()\n        self.raw_text_valid = self.X_valid[self.tweet_column].str.lower()\n        if self.model_Type != \"Spam-Detection\" :\n            self.raw_test_valid = self.test[self.tweet_column].str.lower()\n        return\n\n    def tokenizer(self):\n            '''\n            Input : raw train and dev set along with in-rawed train and dev set.\n            --------------------------------------------------------------------------------\n            Output: tokenizer object along with padded and sequenced training and validation dataset\n            '''\n            # Article for better undrstanding of Tokenizer : https://machinelearningknowledge.ai/keras-tokenizer-tutorial-with-examples-for-fit_on_texts-texts_to_sequences-texts_to_matrix-sequences_to_matrix/\n            #Tokeinzing the raw training set\n            self.tk.fit_on_texts(self.raw_text_train)\n            print(self.raw_text_train.shape )\n            self.X_train[self.tweet_column] = self.tk.texts_to_sequences(self.raw_text_train)\n            self.X_valid[self.tweet_column] = self.tk.texts_to_sequences(self.raw_text_valid)\n            self.X_train = pad_sequences(self.X_train[self.tweet_column], maxlen = self.max_len)\n            self.X_valid = pad_sequences(self.X_valid[self.tweet_column], maxlen = self.max_len)\n            if self.model_Type != \"Spam-Detection\" :\n                self.test[self.tweet_column] = self.tk.texts_to_sequences(self.raw_test_valid )\n                self.test = pad_sequences(self.test[self.tweet_column], maxlen = self.max_len) #test.comment_seq\n            return\n    \n    def get_coefs(self,word,*arr): \n        '''\n        Creating Embeeding Index which can help further to create embedding matrix for the words in our training dataset vocabulary.\n        This Ebedding index is created from the fastText or Glove.\n        '''\n        return word, np.asarray(arr, dtype='float32')\n    \n    \n    def embeeding_Index_Builder(self, embedding_path):\n        '''\n        Embedding Index correpsonding to embeddding Path\n        Input : embeddig path \n        ----------------------\n        Output : embedding Index\n        '''\n        embedding_index = dict(self.get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n        return embedding_index\n\n    def embeeding_Matrix_Builder(self,embedding_index ):\n        '''\n        Input : tokenizer object , with maximum features and embedding size along with fastText and Glove Embedding Index for building Embeeding Matrix\n        ----------------------------------------------------------------------------------------------------------------------------------------------\n        Output: embedding matix corresponding to FastText and Glove\n        '''\n        # Preparing Our Embeeding matrix from Embedding Index from glove or fastText or any other index. \n        damword_index = self.tk.word_index\n        nb_words = min(self.max_features, len(damword_index))\n        embedding_matrix = np.zeros((nb_words+1, self.embed_size))\n        for word, i in damword_index.items():\n            if i >= self.max_features: continue\n            embedding_vector = embedding_index.get(word)\n            if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n        return embedding_matrix\n    \n    def save_embeeding_index_or_matrix_as_csv(self, embedding_index_or_matrix, arrayType=\"index\", embeddingType=\"FastText\"):\n        # save numpy array embeeding_index as csv file\n        # define data\n        if arrayType!=\"index\":\n            data = copy.deepcopy(embedding_index_or_matrix)\n            # save to csv file\n            data = data.ravel()\n            np.savetxt(f'embedding_{arrayType}_{embeddingType}.csv', data, delimiter=',')\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"wb\")\n            pickle.dump(embedding_index_or_matrix, a_file)\n            a_file.close() \n        \n    def load_embedding_index_or_matrix_from_csv(self, arrayType=\"index\", embeddingType=\"FastText\"):\n        # load numpy array  embeeding_index from csv file\n        # load array\n        if arrayType!=\"index\":\n            data = np.loadtxt(f'embedding_{arrayType}_{embeddingType}.csv', delimiter=',')\n            reshaped_data = None\n\n            if embeddingType==\"FastText\" :\n#                 if arrayType==\"index\":\n                reshaped_data = np.reshape(data,self.embedding_index_fastText_shape )\n#                 else:\n#                     reshaped_data = np.reshape(data,self.embedding_matrix_fastText_shape )\n            elif embeddingType==\"Glove\":\n#                 if arrayType==\"index\":\n                reshaped_data = np.reshape(data,self.embedding_index_glove_shape )\n#                 else:\n#                     reshaped_data = np.reshape(data,self.embedding_matrix_glove_shape )\n            # print the array\n            return reshaped_data\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"rb\")\n            output = pickle.load(a_file)\n            return output\n\n            \n    \n    def save_embeeding_index_or_matrix_as_binary(self, embedding_index_or_matrix , arrayType=\"index\", embeddingType=\"FastText\"):\n        # save numpy array embeeding_index as binary file npy\n        # define data\n        if arrayType!=\"index\":\n            np.save(f'embedding_{arrayType}_{embeddingType}.npy', embedding_index_or_matrix )\n            return\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"wb\")\n            pickle.dump(embedding_index_or_matrix, a_file)\n            a_file.close() \n        \n    def load_embedding_index_or_matrix_from_binary(self, arrayType=\"index\", embeddingType=\"FastText\"):\n        # load numpy array  embeeding_index from npy file\n        # load array\n        if arrayType!=\"index\":\n            data = np.load(f'embedding_{arrayType}_{embeddingType}.csv')\n            return data\n        else:\n            a_file = open(f'embedding_{arrayType}_{embeddingType}.pkl', \"rb\")\n            output = pickle.load(a_file)\n            return output\n\n\n    def preprocessing(self, \n                      validation_size=0.1,\n                      embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n                      embedding_path_glove =\"../input/glove840b300dtxt/glove.840B.300d.txt\",\n                      isEmbeddingIndexFileSaved     =False,\n                      isEmbeddingMatrixFileSaved    = False,\n                      wantToSaveEmbeedingIndexFile  = False,\n                      wantToSaveEmbeedingMatrixFile = False ):\n        self.build_train_test_dev_set()\n        print('Building Trainning and Testing is Done.')\n        self.train_test_splitter(validation_size)\n        print('Spliting Trainning and Testing is Done.')\n        self.tokenizer()\n        print('Tokenizing is Done.')\n        if not isEmbeddingIndexFileSaved :\n            self.embedding_index_fastText = self.embeeding_Index_Builder(embedding_path_fastText)\n            print('Embeeding Index for Fasttext is Done.')\n            self.embedding_index_glove = self.embeeding_Index_Builder(embedding_path_glove)\n            print('Embeeding Index for Glove is Done.')\n            if wantToSaveEmbeedingIndexFile:\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_index_fastText  , arrayType=\"index\", embeddingType=\"FastText\")\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_index_glove  , arrayType=\"index\", embeddingType=\"Glove\")\n        else:\n            self.embedding_index_fastText  = self.load_embedding_index_or_matrix_from_binary(arrayType=\"index\", embeddingType=\"FastText\")\n            self.embedding_index_glove  = self.load_embedding_index_or_matrix_from_binary(arrayType=\"index\", embeddingType=\"Glove\")\n#         self.embedding_index_fastText_shape = self.embedding_index_fastText.shape\n#         self.embedding_index_glove_shape = self.embedding_index_glove.shape\n        \n        if not isEmbeddingMatrixFileSaved :\n            self.embedding_matrix_fastText = self.embeeding_Matrix_Builder( self.embedding_index_fastText )\n            print(f'Embedding Matrix for FastText is Done., with it\\'s shape as {self.embedding_matrix_fastText_shape}')\n            self.embedding_matrix_glove = self.embeeding_Matrix_Builder( self.embedding_index_glove )\n            print(f'Embedding Matrix for Glove is Done with shape as { self.embedding_matrix_glove_shape}')\n            if  wantToSaveEmbeedingMatrixFile:\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_matrix_fastText , arrayType=\"matrix\", embeddingType=\"FastText\")\n                self.save_embeeding_index_or_matrix_as_binary(self.embedding_matrix_glove   , arrayType=\"matrix\", embeddingType=\"Glove\")\n        else:\n            self.embedding_matrix_fastText  = self.load_embedding_index_or_matrix_from_binary(arrayType=\"matrix\", embeddingType=\"FastText\")\n            self.embedding_matrix_glove     = self.load_embedding_index_or_matrix_from_binary(arrayType=\"matrix\", embeddingType=\"Glove\")\n#         self.embedding_matrix_fastText_shape = self.embedding_matrix_fastText.shape\n#         self.embedding_matrix_glove_shape =  self.embedding_matrix_glove.shape \n        return\n    \n#######################################################################################################################################################\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:13:20.025284Z","iopub.execute_input":"2022-02-27T16:13:20.025566Z","iopub.status.idle":"2022-02-27T16:13:20.076852Z","shell.execute_reply.started":"2022-02-27T16:13:20.025537Z","shell.execute_reply":"2022-02-27T16:13:20.076144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ML_Model :\n    def __init__(self, \n                 ModelBaseInstance = Preprocessor( embed_size = 300, \n                                           max_features = 130000, \n                                           max_len = 220, \n                                           model_Type = \"Spam-Detection\"),\n#                  embed_size = 300, \n#                  max_features = 130000, \n#                  max_len = 220, \n#                  model_Type = \"Spam-Detection\", \n                 file_path = \"1\",\n                 n_multiClassificationClasses = 1 ,# if Inappropriate then 6 as they are : \"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"\n                 *args, **kwargs):  # for file_path just pass which idea/framework on which this Model is based upon\n#         super(Preprocessor, self).__init__()\n#         super().__init__( embed_size = 300, \n#                           max_features = 130000, \n#                           max_len = 220, \n#                           model_Type = \"Spam-Detection\")\n        self.modelBase = ModelBaseInstance\n        self.model = None\n        self.history = None\n        self.framework_idea = file_path \n        self.file_path = \"best_model_\" + \"FrameWork\" + self.framework_idea + \"_\" + self.modelBase.model_Type  + \".hdf5\" \n        self.check_point = ModelCheckpoint(self.file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n        self.ra_val = None\n        #Allowing early stopping\n        self.early_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 5)\n        self.n_output_nuerons = n_multiClassificationClasses\n        \n    def change_idea_for_same_object(self,change_idea_to=\"2\", automatic_train = False, *args, **kwargs):\n        earlier_idea = self.framework_idea \n        self.framework_idea = change_idea_to\n        self.file_path = \"best_model_\" + \"FrameWork\" + self.framework_idea + \"_\" + self.modelBase.model_Type + \".hdf5\" \n        self.check_point = ModelCheckpoint(self.file_path, monitor = \"val_loss\", verbose = 1, save_best_only = True, mode = \"min\")\n        print(f\"Framework is changeed succesfully \\n Idea/Frameowkr earlier :  {earlier_idea} \\n Idea/Framework Now {self.framework_idea } : \")\n        print(\"\\n\\n PS: This method will keep in handy when you want to test for multiple idea without doing preprocessing step again.\")\n        # TO-DO  allo training auto to be done for user given parameters\n        if automatic_train:\n            print(f\"Note: The training is requested to be done AUTOMATICALLY as automatic_train flag is {automatic_train}, Note:  This training will be done on default parameters \\n i.e lr = 0.0, \\n lr_d = 0.0, \\n units = 0, \\n dr = 0.0, \\n epochs=10.\")\n            print('OR The parameters which was set for earlier IDEA/FRAMEWORK')\n            self.train_Model()\n        else:\n            print(\"\\t Note : WE NEED TO RE-TRAINED THE MODEL.\\n\\n \\t For Help :: USE METHOD --> train_Model for this in order to train the model on new Idea/Framework. \\n \\t Not training automatically as Automatic train Flag is off.\")\n        return\n    \n    def roc_Auc_Valuation(self):\n        self.ra_val = RocAucEvaluation(validation_data=(self.modelBase.X_valid, self.modelBase.Y_valid), interval = 1)\n        return\n        \n    def build_model_framework1(self, lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n        inp = Input(shape = (self.modelBase.max_len,))\n#         x_fastText = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n#         x_glove = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n        x_fastText = Embedding(self.modelBase.embedding_matrix_fastText.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n        x_glove = Embedding(self.modelBase.embedding_matrix_glove.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n        \n    \n    #Drop-Out Layer\n        x1_fastText = SpatialDropout1D(dr)(x_fastText)\n        x1_glove = SpatialDropout1D(dr)(x_glove)\n        #BI-LSTM BRANCH\n        x_glove_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #BI-GRU BRNACH\n        x_glove_gru = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_gru = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #Convolutional+Pooling Layer This is Optional Can be commnented later if required for testing purposes\n        #Bi-listm\n        x_glove_lstm_conv           = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_lstm)\n        x_glove_lstm_conv_avgPool   = GlobalAveragePooling1D()(x_glove_lstm_conv)\n        x_glove_lstm_conv_maxPool   = GlobalMaxPooling1D()(x_glove_lstm_conv)\n        #concatenating\n        x_glove_lstm_conv_pooled    = concatenate([x_glove_lstm_conv_avgPool, x_glove_lstm_conv_maxPool])\n\n        x_fastText_lstm_conv        = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_lstm)\n        x_fastText_lstm_conv_avgPool= GlobalAveragePooling1D()(x_fastText_lstm_conv)\n        x_fastText_lstm_conv_maxPool= GlobalMaxPooling1D()(x_fastText_lstm_conv)\n        #concatenating\n        x_fastText_lstm_conv_pooled = concatenate([x_fastText_lstm_conv_avgPool, x_fastText_lstm_conv_maxPool])\n        #Bi-gru\n        x_glove_gru_conv            = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_gru)\n        x_glove_gru_conv_avgPool    = GlobalAveragePooling1D()(x_glove_gru_conv)\n        x_glove_gru_conv_maxPool    = GlobalMaxPooling1D()(x_glove_gru_conv)\n        #concatenating\n        x_glove_gru_conv_pooled     = concatenate([x_glove_gru_conv_avgPool, x_glove_gru_conv_maxPool])\n        x_fastText_gru_conv         = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_gru)\n        x_fastText_gru_conv_avgPool = GlobalAveragePooling1D()(x_fastText_gru_conv)\n        x_fastText_gru_conv_maxPool = GlobalMaxPooling1D()(x_fastText_gru_conv)\n        #concatenating\n        x_fastText_gru_conv_pooled  = concatenate([x_fastText_gru_conv_avgPool, x_fastText_gru_conv_maxPool])\n        #Con-Catenating Bi-Lstm branch\n        x_lstm_conv_pooled = concatenate([x_glove_lstm_conv_pooled,  x_fastText_lstm_conv_pooled])\n        #Con-Catenating Bi-Gru branch\n        x_gru_conv_pooled = concatenate([x_glove_gru_conv_pooled,  x_fastText_gru_conv_pooled])\n        #ConCatenating Bi-LSTM and Bi-GRU Branch\n        x_lstm_gru_concatenated = concatenate([x_lstm_conv_pooled, x_gru_conv_pooled])\n        #Passing concatenated output to dense network\n    #     xxxx = Dense(8, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n    #     xxx = Dense(4, activation = \"sigmoid\")(xxxx)\n    #     xx = Dense(2, activation = \"sigmoid\")(xx)\n    #     x = Dense(1, activation = \"sigmoid\")(xx) \n        \n#         if self.model_Type != \"Spam-Detection\" :\n#             assert self.n_output_nuerons==6\n#         else:\n#             assert self.n_output_nuerons==1\n\n        print(self.n_output_nuerons)\n            \n        x = Dense(self.n_output_nuerons, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n        self.model = Model(inputs = inp, outputs = x)\n        self.model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    #     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    \n#         print(\"*************************************************\")\n#         print(f\"Y_train's size/shape  ={self.modelBase.Y_train.shape} \")\n#         print(f\"Y_valid's size/shape  ={self.modelBase.Y_valid.shape} \")  \n        \n        self.history = self.model.fit(self.modelBase.X_train, self.modelBase.Y_train, batch_size = 128, epochs = epochs, validation_data = (self.modelBase.X_valid, self.modelBase.Y_valid), \n                            verbose = 1, callbacks = [self.ra_val, self.check_point, self.early_stop])\n        self.model = load_model(self.file_path)\n        return\n        \n    def build_model_framework2(self,lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n        inp = Input(shape = (self.modelBase.max_len,))\n#         x_fastText = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n#         x_glove = Embedding( self.max_features, self.embed_size, weights = [self.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n\n        x_fastText = Embedding(self.modelBase.embedding_matrix_fastText.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_fastText], trainable = False)(inp)# Let this layer be the FatText input embedding\n        x_glove = Embedding(self.modelBase.embedding_matrix_glove.shape[0], self.modelBase.embed_size, weights = [self.modelBase.embedding_matrix_glove], trainable = False)(inp)# Let this layer be the Glove input embedding\n       \n    #Drop-Out Layer\n        x1_fastText = SpatialDropout1D(dr)(x_fastText)\n        x1_glove = SpatialDropout1D(dr)(x_glove)\n        #BI-LSTM BRANCH\n        x_glove_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_lstm = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #BI-GRU BRNACH\n        x_glove_gru = Bidirectional(GRU(units, return_sequences = True))(x1_glove)\n        x_fastText_gru = Bidirectional(GRU(units, return_sequences = True))(x1_fastText)\n        #Convolutional+Pooling Layer This is Optional Can be commnented later if required for testing purposes\n        #Bi-listm\n        x_glove_lstm_conv           = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_lstm)\n        x_glove_lstm_conv_avgPool   = GlobalAveragePooling1D()(x_glove_lstm_conv)\n        x_glove_lstm_conv_maxPool   = GlobalMaxPooling1D()(x_glove_lstm_conv)\n        #concatenating\n        x_glove_lstm_conv_pooled    = concatenate([x_glove_lstm_conv_avgPool, x_glove_lstm_conv_maxPool])\n\n        x_fastText_lstm_conv        = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_lstm)\n        x_fastText_lstm_conv_avgPool= GlobalAveragePooling1D()(x_fastText_lstm_conv)\n        x_fastText_lstm_conv_maxPool= GlobalMaxPooling1D()(x_fastText_lstm_conv)\n        #concatenating\n        x_fastText_lstm_conv_pooled = concatenate([x_fastText_lstm_conv_avgPool, x_fastText_lstm_conv_maxPool])\n        #Bi-gru\n        x_glove_gru_conv            = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_glove_gru)\n        x_glove_gru_conv_avgPool    = GlobalAveragePooling1D()(x_glove_gru_conv)\n        x_glove_gru_conv_maxPool    = GlobalMaxPooling1D()(x_glove_gru_conv)\n        #concatenating\n        x_glove_gru_conv_pooled     = concatenate([x_glove_gru_conv_avgPool, x_glove_gru_conv_maxPool])\n\n        x_fastText_gru_conv         = Conv1D(int(units/2), kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x_fastText_gru)\n        x_fastText_gru_conv_avgPool = GlobalAveragePooling1D()(x_fastText_gru_conv)\n        x_fastText_gru_conv_maxPool = GlobalMaxPooling1D()(x_fastText_gru_conv)\n        #concatenating\n        x_fastText_gru_conv_pooled  = concatenate([x_fastText_gru_conv_avgPool, x_fastText_gru_conv_maxPool])\n        #Con-Catenating Bi-Lstm branch\n        x_lstm_conv_pooled = concatenate([x_glove_lstm_conv_pooled,  x_fastText_lstm_conv_pooled])\n        #Con-Catenating Bi-Gru branch\n        x_gru_conv_pooled = concatenate([x_glove_gru_conv_pooled,  x_fastText_gru_conv_pooled])\n        #ConCatenating Bi-LSTM and Bi-GRU Branch\n        x_lstm_gru_concatenated = concatenate([x_lstm_conv_pooled, x_gru_conv_pooled])\n        #Passing concatenated output to dense network\n    #     xxxx = Dense(8, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n    #     xxx = Dense(4, activation = \"sigmoid\")(xxxx)\n    #     xx = Dense(2, activation = \"sigmoid\")(xxx)\n    #     x = Dense(1, activation = \"sigmoid\")(xx)\n    \n        print(self.n_output_nuerons)\n\n        x = Dense(self.n_output_nuerons, activation = \"sigmoid\")(x_lstm_gru_concatenated)\n        self.model = Model(inputs = inp, outputs = x)\n        self.model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    #     model.compile(loss = \"binary_crossentropy\", optimizer = adam_v2.Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n        self.history = self.model.fit(self.modelBase.X_train, self.modelBase.Y_train, batch_size = 128, epochs = epochs, validation_data = (self.modelBase.X_valid, self.modelBase.Y_valid), \n                            verbose = 1, callbacks = [self.ra_val, self.check_point, self.early_stop])\n        self.model = load_model(self.file_path)\n        return \n    \n    def build_model_framework3(self,lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0, epochs=10):\n        pass\n    \n    def build_model(self, lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 30):\n        self.roc_Auc_Valuation()\n        if self.framework_idea == \"1\":\n            self.build_model_framework1( lr, lr_d , units, dr ,epochs)\n        elif self.framework_idea == \"2\":\n            self.build_model_framework2( lr, lr_d , units, dr ,epochs)\n        else : \n            self.build_model_framework3( lr, lr_d , units, dr ,epochs)\n        \n    def train_Model(self, lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 30):\n        #For training use this member function\n        print(\"TRAINING STARTED\")\n        self.build_model(lr , lr_d, units, dr ,epochs)\n        print(\"TRAINING COMPLETED\")\n    \n    def get_accuracy_for_validation_set(self):\n        return self.prediction(self.modelBase.X_valid, self.modelBase.Y_valid)\n        \n    def get_accuracy_for_training_set(self):\n        return self.prediction(self.modelBase.X_train, self.modelBase.Y_train)\n         \n    def Plot(self, string): # example Object.Plot(history, \"accuracy\")  or Object.Plot(history, \"loss\")\n        plt.plot(self.history.history[string])\n        plt.plot(self.history.history['val_' + string])\n        plt.xlabel(\"EPOCHS\")\n        plt.ylabel(string)\n        plt.legend([string, 'val_' + string ])\n        plt.savefig(string + \"_framework-\" + self.framework_idea  +'_' + self.modelBase.model_Type  + '_'+'.png')\n        plt.show()       \n        print(f\"###### DONE PLOTTING FOR IDEA - {self.framework_idea} ########\")\n                    \n    def prediction(self,dataset, y_actual,on_the_fly=0 ): # On the fly is for those data that is comimg on the fly from any tweet.\n        if on_the_fly == 1 :\n            pass\n        y_pred_validation = self.model.predict(dataset, verbose=1)\n        score_validation = roc_auc_score(y_actual, y_pred_validation)\n        print(f\"\\n ROC-AUC - ON Validation Dataset - score: {round(score_validation,5)*100}%\")\n        return y_pred_validation\n    \n##############################################################################################################################################################","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:13:26.16506Z","iopub.execute_input":"2022-02-27T16:13:26.165324Z","iopub.status.idle":"2022-02-27T16:13:26.216138Z","shell.execute_reply.started":"2022-02-27T16:13:26.165294Z","shell.execute_reply":"2022-02-27T16:13:26.215459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Inappropriate Content Detection**","metadata":{}},{"cell_type":"code","source":"%%time\nInappContentModel1Preprocessor = Preprocessor( embed_size = 300, \n                                               max_features = 130000, \n                                               max_len = 220, \n                                               model_Type = \"Inappropriate-Content-Detection\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T13:56:13.775165Z","iopub.execute_input":"2022-02-27T13:56:13.775883Z","iopub.status.idle":"2022-02-27T13:56:13.783038Z","shell.execute_reply.started":"2022-02-27T13:56:13.775844Z","shell.execute_reply":"2022-02-27T13:56:13.782148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nInappContentModel1Preprocessor.preprocessing( validation_size=0.1,\n                                  embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n                                  embedding_path_glove =\"../input/glove840b300dtxt/glove.840B.300d.txt\" ,\n                                  isEmbeddingIndexFileSaved     =False,\n                                  isEmbeddingMatrixFileSaved    = False,\n                                  wantToSaveEmbeedingIndexFile  = False,\n                                  wantToSaveEmbeedingMatrixFile = False )","metadata":{"execution":{"iopub.status.busy":"2022-02-27T13:56:43.407326Z","iopub.execute_input":"2022-02-27T13:56:43.407905Z","iopub.status.idle":"2022-02-27T14:03:16.682944Z","shell.execute_reply.started":"2022-02-27T13:56:43.407868Z","shell.execute_reply":"2022-02-27T14:03:16.682088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(InappContentModel1Preprocessor.X_valid)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# InappContentModel1Preprocessor.test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-27T12:11:21.202269Z","iopub.execute_input":"2022-02-27T12:11:21.202813Z","iopub.status.idle":"2022-02-27T12:11:21.208239Z","shell.execute_reply.started":"2022-02-27T12:11:21.202777Z","shell.execute_reply":"2022-02-27T12:11:21.207496Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ******************************************************","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nInappContentModel1 = ML_Model( InappContentModel1Preprocessor,\n                               file_path = \"1\",\n                               n_multiClassificationClasses = 6 )","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:03:16.685857Z","iopub.execute_input":"2022-02-27T14:03:16.686063Z","iopub.status.idle":"2022-02-27T14:03:16.693908Z","shell.execute_reply.started":"2022-02-27T14:03:16.686036Z","shell.execute_reply":"2022-02-27T14:03:16.693052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nInappContentModel1.train_Model(lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 3 )\n\n","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:03:16.695213Z","iopub.execute_input":"2022-02-27T14:03:16.695475Z","iopub.status.idle":"2022-02-27T14:13:53.778245Z","shell.execute_reply.started":"2022-02-27T14:03:16.695439Z","shell.execute_reply":"2022-02-27T14:13:53.777539Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(InappContentModel1.n_output_nuerons )\nprint(InappContentModel1.modelBase.embedding_matrix_glove.shape[0])\nprint(InappContentModel1.modelBase.max_features)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:13:53.782788Z","iopub.execute_input":"2022-02-27T14:13:53.784996Z","iopub.status.idle":"2022-02-27T14:13:53.794614Z","shell.execute_reply.started":"2022-02-27T14:13:53.784954Z","shell.execute_reply":"2022-02-27T14:13:53.793931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred_train = InappContentModel1.get_accuracy_for_training_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:13:53.799222Z","iopub.execute_input":"2022-02-27T14:13:53.80133Z","iopub.status.idle":"2022-02-27T14:16:20.30085Z","shell.execute_reply.started":"2022-02-27T14:13:53.801292Z","shell.execute_reply":"2022-02-27T14:16:20.299029Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred_valid  = InappContentModel1.get_accuracy_for_validation_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:16:20.302097Z","iopub.execute_input":"2022-02-27T14:16:20.302376Z","iopub.status.idle":"2022-02-27T14:16:40.85201Z","shell.execute_reply.started":"2022-02-27T14:16:20.302325Z","shell.execute_reply":"2022-02-27T14:16:40.851201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappContentModel1.Plot( \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:16:40.853766Z","iopub.execute_input":"2022-02-27T14:16:40.854306Z","iopub.status.idle":"2022-02-27T14:16:41.273063Z","shell.execute_reply.started":"2022-02-27T14:16:40.854263Z","shell.execute_reply":"2022-02-27T14:16:41.272416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"InappContentModel1.Plot( \"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:16:41.276766Z","iopub.execute_input":"2022-02-27T14:16:41.278635Z","iopub.status.idle":"2022-02-27T14:16:41.570864Z","shell.execute_reply.started":"2022-02-27T14:16:41.278592Z","shell.execute_reply":"2022-02-27T14:16:41.570117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# CHECKING FOR IDEA 2 BY SWITCHING \n\nInappContentModel1.change_idea_for_same_object( change_idea_to = \"2\", automatic_train = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:16:41.57213Z","iopub.execute_input":"2022-02-27T14:16:41.572776Z","iopub.status.idle":"2022-02-27T14:40:18.144656Z","shell.execute_reply.started":"2022-02-27T14:16:41.572737Z","shell.execute_reply":"2022-02-27T14:40:18.143853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2\ny_pred_train = InappContentModel1.get_accuracy_for_training_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:40:18.14724Z","iopub.execute_input":"2022-02-27T14:40:18.147806Z","iopub.status.idle":"2022-02-27T14:43:42.46422Z","shell.execute_reply.started":"2022-02-27T14:40:18.147764Z","shell.execute_reply":"2022-02-27T14:43:42.463418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2 \ny_pred_valid  = InappContentModel1.get_accuracy_for_validation_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:43:42.465633Z","iopub.execute_input":"2022-02-27T14:43:42.465914Z","iopub.status.idle":"2022-02-27T14:44:03.00186Z","shell.execute_reply.started":"2022-02-27T14:43:42.465876Z","shell.execute_reply":"2022-02-27T14:44:03.001048Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA 2\nInappContentModel1.Plot( \"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:44:03.003279Z","iopub.execute_input":"2022-02-27T14:44:03.003563Z","iopub.status.idle":"2022-02-27T14:44:03.253486Z","shell.execute_reply.started":"2022-02-27T14:44:03.003526Z","shell.execute_reply":"2022-02-27T14:44:03.252422Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA 2 \nInappContentModel1.Plot( \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T14:44:03.497612Z","iopub.execute_input":"2022-02-27T14:44:03.498441Z","iopub.status.idle":"2022-02-27T14:44:03.738667Z","shell.execute_reply.started":"2022-02-27T14:44:03.498401Z","shell.execute_reply":"2022-02-27T14:44:03.737971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**SPAM detection**\n","metadata":{}},{"cell_type":"code","source":"%%time\nspamDetectionModel1Preprocessor = Preprocessor( embed_size = 300, \n                                               max_features = 130000, \n                                               max_len = 220, \n                                               model_Type = \"Spam-Detection\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:13:53.091639Z","iopub.execute_input":"2022-02-27T16:13:53.092351Z","iopub.status.idle":"2022-02-27T16:13:53.097554Z","shell.execute_reply.started":"2022-02-27T16:13:53.092313Z","shell.execute_reply":"2022-02-27T16:13:53.096831Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nspamDetectionModel1Preprocessor.preprocessing( validation_size=0.1,\n                                  embedding_path_fastText = \"../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec\",\n                                  embedding_path_glove =\"../input/glove840b300dtxt/glove.840B.300d.txt\" ,\n                                  isEmbeddingIndexFileSaved     =False,\n                                  isEmbeddingMatrixFileSaved    = False,\n                                  wantToSaveEmbeedingIndexFile  = False,\n                                  wantToSaveEmbeedingMatrixFile = False )","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:13:55.501561Z","iopub.execute_input":"2022-02-27T16:13:55.502039Z","iopub.status.idle":"2022-02-27T16:19:40.270078Z","shell.execute_reply.started":"2022-02-27T16:13:55.502005Z","shell.execute_reply":"2022-02-27T16:19:40.269235Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nspamDetectionModel1 = ML_Model( spamDetectionModel1Preprocessor,\n                               file_path = \"1\",\n                               n_multiClassificationClasses = 1 )","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:22:34.125815Z","iopub.execute_input":"2022-02-27T16:22:34.126075Z","iopub.status.idle":"2022-02-27T16:22:34.131948Z","shell.execute_reply.started":"2022-02-27T16:22:34.126046Z","shell.execute_reply":"2022-02-27T16:22:34.130973Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\nspamDetectionModel1.train_Model(lr = 1e-3, lr_d = 0, units = 112, dr = 0.2,epochs = 3 )","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:22:36.755106Z","iopub.execute_input":"2022-02-27T16:22:36.755388Z","iopub.status.idle":"2022-02-27T16:23:32.418932Z","shell.execute_reply.started":"2022-02-27T16:22:36.755358Z","shell.execute_reply":"2022-02-27T16:23:32.417434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\ny_pred_valid  = spamDetectionModel1.get_accuracy_for_validation_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:23:32.420605Z","iopub.execute_input":"2022-02-27T16:23:32.420805Z","iopub.status.idle":"2022-02-27T16:23:35.580771Z","shell.execute_reply.started":"2022-02-27T16:23:32.42078Z","shell.execute_reply":"2022-02-27T16:23:35.580006Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spamDetectionModel1.Plot( \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:23:35.582124Z","iopub.execute_input":"2022-02-27T16:23:35.582572Z","iopub.status.idle":"2022-02-27T16:23:35.889317Z","shell.execute_reply.started":"2022-02-27T16:23:35.582535Z","shell.execute_reply":"2022-02-27T16:23:35.888572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"spamDetectionModel1.Plot( \"accuracy\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:23:35.891473Z","iopub.execute_input":"2022-02-27T16:23:35.891929Z","iopub.status.idle":"2022-02-27T16:23:36.156734Z","shell.execute_reply.started":"2022-02-27T16:23:35.891891Z","shell.execute_reply":"2022-02-27T16:23:36.155986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# CHECKING FOR IDEA 2 BY SWITCHING \nspamDetectionModel1.change_idea_for_same_object( change_idea_to = \"2\", automatic_train = True)","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:23:36.157911Z","iopub.execute_input":"2022-02-27T16:23:36.15874Z","iopub.status.idle":"2022-02-27T16:28:12.021592Z","shell.execute_reply.started":"2022-02-27T16:23:36.158695Z","shell.execute_reply":"2022-02-27T16:28:12.020736Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2\ny_pred_train = spamDetectionModel1.get_accuracy_for_training_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:28:12.022815Z","iopub.execute_input":"2022-02-27T16:28:12.02306Z","iopub.status.idle":"2022-02-27T16:28:25.657961Z","shell.execute_reply.started":"2022-02-27T16:28:12.023023Z","shell.execute_reply":"2022-02-27T16:28:25.65712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n# Note this prediction is  FOR IDEA 2 \ny_pred_valid  = spamDetectionModel1.get_accuracy_for_validation_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:28:25.659305Z","iopub.execute_input":"2022-02-27T16:28:25.659572Z","iopub.status.idle":"2022-02-27T16:28:26.97196Z","shell.execute_reply.started":"2022-02-27T16:28:25.659537Z","shell.execute_reply":"2022-02-27T16:28:26.9711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA 2 \nspamDetectionModel1.Plot( \"loss\")","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:28:26.973515Z","iopub.execute_input":"2022-02-27T16:28:26.973788Z","iopub.status.idle":"2022-02-27T16:28:27.395089Z","shell.execute_reply.started":"2022-02-27T16:28:26.973749Z","shell.execute_reply":"2022-02-27T16:28:27.394432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Note this prediction is  FOR IDEA 2\nspamDetectionModel1.Plot( \"accuracy\")\n\n%%time\n# Note this prediction is  FOR IDEA 2\ny_pred_train = InappContentModel1.get_accuracy_for_training_set()","metadata":{"execution":{"iopub.status.busy":"2022-02-27T16:28:27.398776Z","iopub.execute_input":"2022-02-27T16:28:27.399377Z","iopub.status.idle":"2022-02-27T16:28:27.730125Z","shell.execute_reply.started":"2022-02-27T16:28:27.399337Z","shell.execute_reply":"2022-02-27T16:28:27.728842Z"},"trusted":true},"execution_count":null,"outputs":[]}]}