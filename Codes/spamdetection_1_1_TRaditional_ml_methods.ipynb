{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpamDetection_1.1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Spam detection using ML models\n",
        "####By: Pintu (181co139), Awanit (181co161),Vivek (181co159), \n",
        "####"
      ],
      "metadata": {
        "id": "0K9EuvmOCWvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "mount Googledrive\n"
      ],
      "metadata": {
        "id": "048jq3lWWg-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeyajzBzWV9N",
        "outputId": "7d13adf7-9a29-4e49-8724-cabc3393e0f8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read the dataset"
      ],
      "metadata": {
        "id": "ttrji0eyV-WH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 29/01/22\n",
        "from sklearn import preprocessing\n",
        "import pandas as pd\n",
        "\n",
        "dataset=pd.read_csv('/content/drive/MyDrive/Colab datasets/Spam_dataset.csv', header=0, delimiter=\",\")\n",
        "\n",
        "#remove the missing data.\n",
        "dataset=dataset.dropna()\n",
        "print(\"Head of the dataset =>\\n\",dataset.head()) #dataset.tail()\n",
        "\n",
        "#Category -> Integer\n",
        "#Ham = 0\n",
        "#Spam = 1\n",
        "# le = preprocessing.LabelEncoder()\n",
        "# le.fit(dataset.Category)\n",
        "# dataset['Category'] = le.transform(dataset.Category)\n",
        "\n",
        "spam_cnt=0\n",
        "for i in range(0, len(dataset['Category'])):\n",
        "  if (dataset['Category'][i]=='ham'):\n",
        "    dataset['Category'][i]=\"0\"\n",
        "  else:\n",
        "    dataset['Category'][i]=\"1\"\n",
        "    spam_cnt+=1\n",
        "\n",
        "\n",
        "print(\"\\nTail of dataset after labelling =>\\n\",dataset.tail())\n",
        "print(\"\\nTotal spam texts are =>\", spam_cnt,\" out of \",len(dataset))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0yBgYyQJV5ky",
        "outputId": "2db79d18-43da-46da-8f3f-a9d2966a14e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Head of the dataset =>\n",
            "   Category                                            Message\n",
            "0      ham  Go until jurong point, crazy.. Available only ...\n",
            "1      ham                      Ok lar... Joking wif u oni...\n",
            "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
            "3      ham  U dun say so early hor... U c already then say...\n",
            "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
            "\n",
            "Tail of dataset after labelling =>\n",
            "      Category                                            Message\n",
            "5567        1  This is the 2nd time we have tried 2 contact u...\n",
            "5568        0               Will Ã¼ b going to esplanade fr home?\n",
            "5569        0  Pity, * was in mood for that. So...any other s...\n",
            "5570        0  The guy did some bitching but I acted like i'd...\n",
            "5571        0                         Rofl. Its true to its name\n",
            "\n",
            "Total spam texts are => 747  out of  5572\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PreProcessing\n"
      ],
      "metadata": {
        "id": "x4Zn63h_cxJx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 29/01/22\n",
        "#Preprocessing of the dataset step\n",
        "\n",
        "#PUNCTUATION \n",
        "#library that contains punctuation\n",
        "import string\n",
        "string.punctuation\n",
        "#defining the function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree\n",
        "\n",
        "#TOLOWER\n",
        "#converting text into lower format.\n",
        "def to_lower(text):\n",
        "    lowered_text=text.lower()\n",
        "    return lowered_text\n",
        "\n",
        "#TOKENIZATION\n",
        "#defining function for tokenization\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenization(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "#REMOVE STOPWORDS\n",
        "#importing nlp library\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "#Stop words present in the library\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "print(\"\\nFew english stopwords are: \",stopwords[0:10])\n",
        "\n",
        "#defining the function to remove stopwords from tokenized text\n",
        "def remove_stopwords(text):\n",
        "    output= [i for i in text if i not in stopwords]\n",
        "    return output\n",
        "\n",
        "#STEMMING\n",
        "#importing the Stemming function from nltk library\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "#defining a function for stemming\n",
        "def stemming(text):\n",
        "    stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text\n",
        "\n",
        "#LEMMATIZATION\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "    lemmed_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemmed_text\n",
        "\n",
        "\n",
        "##PRE-PROCESSING##\n",
        "def text_preprocessing(text):\n",
        "    punctuation_free=remove_punctuation(text)\n",
        "    lower_text=to_lower(punctuation_free)\n",
        "    tokenized_text=tokenization(lower_text)\n",
        "    stopwords_free=remove_stopwords(tokenized_text)\n",
        "    # stemmed_text=stemming(stopwords_free)\n",
        "\n",
        "    ## TODO : In order to improve word root removing stemming\n",
        "    stemmed_text = stopwords_free\n",
        "    lemmatized_text=lemmatizer(stemmed_text)\n",
        "    return [punctuation_free,lower_text,tokenized_text,stopwords_free,stemmed_text,lemmatized_text]\n",
        "\n",
        "#Apply pre-processing function.\n",
        "for i in range(0,len(dataset['Message'])):\n",
        "  dataset['Message'][i]=text_preprocessing(dataset['Message'][i])[5]\n",
        "\n",
        "#Many other steps include: URL removal, HTML tags removal, Rare words removal, Frequent words removal, Spelling checking, and many more.\n",
        "#But we are not going for further pre-processing cause we need to detect spams and spam contains URL."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyTlMdgjWOIJ",
        "outputId": "9ce2649d-f97d-48a3-a307-fe076901caef"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "\n",
            "Few english stopwords are:  ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify pre-processing"
      ],
      "metadata": {
        "id": "VyyzZC6PYH0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 29/01/22\n",
        "#verify.\n",
        "\n",
        "text=\"Let's See; CONNECTION Misunderstanding swimming; ASSure the Authentication: of Working of tExt Pre-processing\"\n",
        "[punctuation_free,lower_text,tokenized_text,stopwords_free,stemmed_text,lemmatized_text]=text_preprocessing(text)\n",
        "print(\"Remove punctuation=> \",punctuation_free)\n",
        "print(\"Lowered text      => \",lower_text)\n",
        "print(\"Tokens separated  => \",tokenized_text)\n",
        "print(\"Remove stopwords  => \",stopwords_free)\n",
        "print(\"Stemming          => \",stemmed_text)\n",
        "print(\"Lemmatizer        => \",lemmatized_text)\n",
        "\n",
        "print(\"\\nVerify the proper preprocessing:\\n\",dataset['Message'][0:7])\n",
        "\n",
        "#success msg.\n",
        "print(\"\\nAll texts have been preprocessed successfully!!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcO8eoE-YHCh",
        "outputId": "e7a0389a-795a-4efc-9484-80c7be796ee4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remove punctuation=>  Lets See CONNECTION Misunderstanding swimming ASSure the Authentication of Working of tExt Preprocessing\n",
            "Lowered text      =>  lets see connection misunderstanding swimming assure the authentication of working of text preprocessing\n",
            "Tokens separated  =>  ['lets', 'see', 'connection', 'misunderstanding', 'swimming', 'assure', 'the', 'authentication', 'of', 'working', 'of', 'text', 'preprocessing']\n",
            "Remove stopwords  =>  ['lets', 'see', 'connection', 'misunderstanding', 'swimming', 'assure', 'authentication', 'working', 'text', 'preprocessing']\n",
            "Stemming          =>  ['lets', 'see', 'connection', 'misunderstanding', 'swimming', 'assure', 'authentication', 'working', 'text', 'preprocessing']\n",
            "Lemmatizer        =>  ['let', 'see', 'connection', 'misunderstanding', 'swimming', 'assure', 'authentication', 'working', 'text', 'preprocessing']\n",
            "\n",
            "Verify the proper preprocessing:\n",
            " 0    [go, jurong, point, crazy, available, bugis, n...\n",
            "1                       [ok, lar, joking, wif, u, oni]\n",
            "2    [free, entry, 2, wkly, comp, win, fa, cup, fin...\n",
            "3        [u, dun, say, early, hor, u, c, already, say]\n",
            "4    [nah, dont, think, go, usf, life, around, though]\n",
            "5    [freemsg, hey, darling, 3, week, word, back, i...\n",
            "6    [even, brother, like, speak, treat, like, aid,...\n",
            "Name: Message, dtype: object\n",
            "\n",
            "All texts have been preprocessed successfully!!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create Vocabulary"
      ],
      "metadata": {
        "id": "CodhyaMVgLWA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pintu 30/01/22\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer                    \n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "x=dataset['Message']\n",
        "y=dataset['Category']\n",
        "\n",
        "#create the dict.\n",
        "tokenizer = Tokenizer(num_words=8000)\n",
        "tokenizer.fit_on_texts(x)\n",
        "\n",
        "#number of unique words in dict.\n",
        "print(\"Number of unique words in dictionary=\",len(tokenizer.word_index))\n",
        "\n",
        "#replace words with their index in vocab.\n",
        "x_train = tokenizer.texts_to_sequences(x)\n",
        "\n",
        "\n",
        "# Adding 1 because of  reserved 0 index\n",
        "vocab_size = len(tokenizer.word_index) + 1  \n",
        "\n",
        "#size of random text in training set.\n",
        "print(\"Length of random text=\")\n",
        "for i in range(10):\n",
        "  print(\"Length of \",i+1,\" text is => \",len(x_train[i]))\n",
        "\n",
        "#Maximum length of each text\n",
        "maxlen = 30\n",
        "\n",
        "#pad the short text and truncate longer texts.\n",
        "x_train = pad_sequences(x_train, padding='post', maxlen=maxlen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iV6RDKhOOnSP",
        "outputId": "d19c6f83-91e6-4828-8407-37162ef988cf"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words in dictionary= 8973\n",
            "Length of random text=\n",
            "Length of  1  text is =>  16\n",
            "Length of  2  text is =>  6\n",
            "Length of  3  text is =>  23\n",
            "Length of  4  text is =>  9\n",
            "Length of  5  text is =>  8\n",
            "Length of  6  text is =>  19\n",
            "Length of  7  text is =>  8\n",
            "Length of  8  text is =>  16\n",
            "Length of  9  text is =>  18\n",
            "Length of  10  text is =>  18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Confirm that text is converted into vector"
      ],
      "metadata": {
        "id": "V2Pnhfm5hVcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pintu 30/01/22\n",
        "print(\"First tweet=> \",x[0])\n",
        "print(\"Text to seq=> \",x_train[1])\n",
        "print(\"\\nVerification successful!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8O8yhi2AQJbJ",
        "outputId": "d72a5d66-a60c-4d80-b425-82e3badaec39"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First tweet=>  ['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat']\n",
            "Text to seq=>  [  10  233 1260  352    1 1660    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "\n",
            "Verification successful!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMOTE: Synthetic Minority Oversampling TEchnique"
      ],
      "metadata": {
        "id": "1KwlP5caQkda"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline\n",
        "#print(imblearn.__version__)\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "msg,y= x_train, dataset['Category'].to_numpy()\n",
        "print(\"Type of message column and class label => \",type(msg),type(y))\n",
        "count = Counter(y)\n",
        "print(\"\\nInitial Count of each class is =>\",count)\n",
        "\n",
        "# over = SMOTE(sampling_strategy=0.2) #sampling_strategy argument tells that minority = 10% of majority.\n",
        "# X, y = over.fit_resample(X, y)\n",
        "# under = RandomUnderSampler(sampling_strategy=0.5) #sampling_strategy argument tells that majority = 1/0.5 of minority.\n",
        "# X, y = under.fit_resample(X, y)\n",
        "\n",
        "#Using Pipeline instead of doing separately.\n",
        "over = SMOTE(sampling_strategy=0.2)\n",
        "under = RandomUnderSampler(sampling_strategy=0.5)\n",
        "steps_in_order = [('o', over), ('u', under)]\n",
        "pipeline = Pipeline(steps=steps_in_order)\n",
        "\n",
        "# transform the dataset\n",
        "X, y = pipeline.fit_resample(msg, y)\n",
        "# summarize the new class distribution\n",
        "counter = Counter(y)\n",
        "print(\"\\nNew count after SMOTE of both class =>\",counter)\n",
        "print(\"\\nSize of X and y => \",len(X),len(y))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V66VNaXsQkCe",
        "outputId": "5325aa21-95bd-415f-c753-8c61a9944c24"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of message column and class label =>  <class 'numpy.ndarray'> <class 'numpy.ndarray'>\n",
            "\n",
            "Initial Count of each class is => Counter({'0': 4825, '1': 747})\n",
            "\n",
            "New count after SMOTE of both class => Counter({'0': 1930, '1': 965})\n",
            "\n",
            "Size of X and y =>  2895 2895\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Download zip glove file"
      ],
      "metadata": {
        "id": "opIXMwzdhh0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqYbN3pRTkk_",
        "outputId": "4350de16-7c9e-483d-e999-ce26e776fa6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-01-31 10:47:40--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-01-31 10:47:41--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-01-31 10:47:42--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: âglove.6B.zipâ\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.07MB/s    in 2m 41s  \n",
            "\n",
            "2022-01-31 10:50:24 (5.09 MB/s) - âglove.6B.zipâ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzip glove file"
      ],
      "metadata": {
        "id": "zvtfxeO9hprL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip glove*.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7q1sN_-To0q",
        "outputId": "9f77da98-024c-4c81-ddb9-02f60ed2ed78"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embeddings of vocab."
      ],
      "metadata": {
        "id": "qWAgKLqr4WS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "import numpy as np\n",
        "\n",
        "#vocab: 'the': 1, mapping of words with integers in seq. 1,2,3..\n",
        "#embedding: 1->dense vector\n",
        "\n",
        "def embedding_for_vocab(filepath, word_index, embedding_dim):\n",
        "    vocab_size = len(word_index) + 1  \n",
        "    # Adding again 1 because of reserved 0 index\n",
        "    embedding_matrix_vocab = np.zeros((vocab_size, embedding_dim))\n",
        "  \n",
        "    with open(filepath) as f:\n",
        "        for line in f:\n",
        "            word, *vector = line.split()\n",
        "            if word in word_index:\n",
        "                idx = word_index[word] \n",
        "                embedding_matrix_vocab[idx] = np.array(vector, dtype=np.float32)[:embedding_dim]\n",
        "\n",
        "    return embedding_matrix_vocab\n",
        "\n",
        "#matrix for vocab: word_index\n",
        "embedding_dim = 50\n",
        "embedding_matrix_vocab = embedding_for_vocab('/content/glove.6B.50d.txt',tokenizer.word_index,embedding_dim)\n"
      ],
      "metadata": {
        "id": "s_JXnoME4VcI"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the vocabulary and embedding matrix."
      ],
      "metadata": {
        "id": "jz8GWlVE5N3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "print(\"Type of vocabulary => \",type(tokenizer.word_index))\n",
        "print(\"\\nVocabulary glance => \",)\n",
        "stop=0\n",
        "\n",
        "#see the dict.\n",
        "for word,index in tokenizer.word_index.items():\n",
        "  stop+=1\n",
        "  if(stop==10):\n",
        "    break\n",
        "  print(index,\" => \",word)\n",
        "  \n",
        "#dense vector.\n",
        "print(\"\\nDense vector of first word in dict => \\n\",embedding_matrix_vocab[1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mAzpXnvQ5GXn",
        "outputId": "6f712bf2-3b59-433b-ca36-2a0a9d0994cc"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of vocabulary =>  <class 'dict'>\n",
            "\n",
            "Vocabulary glance => \n",
            "1  =>  u\n",
            "2  =>  call\n",
            "3  =>  2\n",
            "4  =>  im\n",
            "5  =>  get\n",
            "6  =>  ur\n",
            "7  =>  go\n",
            "8  =>  4\n",
            "9  =>  dont\n",
            "\n",
            "Dense vector of first word in dict => \n",
            " [-0.25676     0.8549      1.10029995  0.95362997  0.36585    -1.30289996\n",
            "  1.07539999 -0.18460999 -0.67673999  0.37637001 -0.029637    0.51697999\n",
            " -0.19248    -0.41863    -0.71144003  0.12564    -0.42965001  0.61456001\n",
            "  0.41819     0.27605999 -0.48635    -0.32585001  0.67747998  0.15916\n",
            "  0.35051    -0.29392999 -0.80439001 -0.15939     0.012475   -0.58403999\n",
            "  2.13529992 -0.1547     -0.57389998  1.45220006  0.6124     -0.68752003\n",
            "  1.28390002 -0.54631001 -0.35736999  0.57323003  0.35460001 -0.37465\n",
            " -0.74628001 -0.074561   -0.48471001  0.067343   -0.039338   -0.22177\n",
            "  0.099708    0.55553001]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create embedding for all tweet texts."
      ],
      "metadata": {
        "id": "8VQLWEV14g--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#tweet_text: [23, 0, 34, ..., 35] tweet converted into seq.\n",
        "#goal: [[dense vector for 23], ..., [dense vecor for 35]]\n",
        "def embedded_tweet_text(embedding_matrix_vocab, tweet_text):\n",
        "    matrix_size = len(tweet_text)   \n",
        "    \n",
        "    tweet_text_matrix = np.zeros((matrix_size, embedding_dim))\n",
        "    \n",
        "    #traverse pad_seq of each tweet text\n",
        "    for i in range(0,matrix_size):\n",
        "      index=tweet_text[i]\n",
        "      if(index==0):\n",
        "        continue\n",
        "      else:\n",
        "        tweet_text_matrix[i]=embedding_matrix_vocab[index]\n",
        "    return tweet_text_matrix\n",
        "\n",
        "#define zero matrix.\n",
        "n=len(X)\n",
        "x1=np.zeros((n, 30, 50))\n",
        "\n",
        "#populate matrix.\n",
        "for i in range(0,n):\n",
        "  x1[i]=embedded_tweet_text(embedding_matrix_vocab, X[i])\n",
        "\n",
        "#ML model don't accept input in 3D shape\n",
        "#so reshape 3D into 2D: 1D array per tweet.\n",
        "x2=x1.reshape(len(X),-1)\n",
        "print(\"Shape of x2 => \",x2.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUNt5zL94gWj",
        "outputId": "29a898c6-344b-45d2-fd38-8200102a5624"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of x2 =>  (2895, 1500)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifying whole embedding."
      ],
      "metadata": {
        "id": "ZDnAnHkB6vnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#x <- preprocessed\n",
        "#x_train <- padded seq\n",
        "#X <- smoted\n",
        "#x1 <- final embedding\n",
        "\n",
        "print(\"First tweet as text =>\\n\",dataset['Message'][1])\n",
        "print(\"First tweet: text->seq =>\\n\",x_train[1])\n",
        "print(\"Dense vector for 1st word of above tweet =>\\n\",embedding_matrix_vocab[22])\n",
        "print(\"First tweet -> 2D matrix of dense vectors =>\\n\",x1[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zoHD7ZKU6IEb",
        "outputId": "475771af-6851-4999-d1d5-3300afc058a9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First tweet as text =>\n",
            " ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
            "First tweet: text->seq =>\n",
            " [  10  233 1260  352    1 1660    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n",
            "Dense vector for 1st word of above tweet =>\n",
            " [ 0.13627    -0.054478    0.37029999 -0.41574001  0.60567999 -0.42728999\n",
            " -0.50151002  0.35923001 -0.49154001  0.21827    -0.15193     0.52535999\n",
            " -0.24206001  0.023875    0.82249999  1.08899999  0.98825002 -0.17803\n",
            "  0.77806002 -1.06470001 -0.28742     0.50458002  0.21612     0.65680999\n",
            "  0.34294999 -2.10840011 -0.82556999 -0.31966001  0.87567002 -1.06789994\n",
            "  3.38019991  1.20840001 -1.27199996 -0.15921    -0.25237    -0.2696\n",
            " -0.18756001 -0.35523     0.084172   -0.56538999 -0.24081001  0.15926\n",
            "  0.32870001  0.54591     0.29897001  0.18948001 -0.57112998  0.17399\n",
            " -0.19338     0.51920998]\n",
            "First tweet -> 2D matrix of dense vectors =>\n",
            " [[-0.38497001  0.80092001  0.064106   ... -0.57653999  0.048833\n",
            "   0.67203999]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.28318     1.04009998 -0.42103001 ...  0.12396     0.95081002\n",
            "   0.13556001]\n",
            " ...\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]\n",
            " [ 0.          0.          0.         ...  0.          0.\n",
            "   0.        ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build Models: k-fold cross validation using accuracy as metric."
      ],
      "metadata": {
        "id": "C1qwIGh3UXO7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Pintu 30/01/22\n",
        "\n",
        "#from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from numpy import mean\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "# X and y are being SMOTE\n",
        "\n",
        "folds = range(3,10)\n",
        "\n",
        "# define the model to be evaluate\n",
        "\n",
        "# get a list of models to evaluate\n",
        "def get_models():\n",
        "\tmodels = list()\n",
        "\tmodels.append(LogisticRegression())\n",
        "\tmodels.append(SGDClassifier())\n",
        "\tmodels.append(KNeighborsClassifier())\n",
        "\tmodels.append(DecisionTreeClassifier())\n",
        "\tmodels.append(SVC())\n",
        "\tmodels.append(GaussianNB())\n",
        "\tmodels.append(RandomForestClassifier())\n",
        "\tmodels.append(GradientBoostingClassifier())\n",
        "\treturn models\n",
        "\n",
        "models = get_models()\n",
        "# evaluate each k value for a model\n",
        "def evaluate_model(model):\n",
        "  for k in folds:\n",
        "    cross_validation = KFold(n_splits=k, shuffle=True, random_state=1)\n",
        "    scores = cross_val_score(model, x2, y, scoring='accuracy', cv=cross_validation, n_jobs=-1)\n",
        "    print(\"Average of the accuracy for \",k,\"folds => \",mean(scores))\n",
        "\n",
        "# evaluate each model\n",
        "for m in models:\n",
        "  model=m\n",
        "  print(\"\\nSummary of model:\",model)\n",
        "  evaluate_model(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOriH0WIGk5x",
        "outputId": "1160c62b-e196-483b-c188-2b93fec258b0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summary of model: LogisticRegression()\n",
            "Average of the accuracy for  3 folds =>  0.8756476683937824\n",
            "Average of the accuracy for  4 folds =>  0.8773746207866242\n",
            "Average of the accuracy for  5 folds =>  0.8766839378238341\n",
            "Average of the accuracy for  6 folds =>  0.8832504316899049\n",
            "Average of the accuracy for  7 folds =>  0.8804861664636378\n",
            "Average of the accuracy for  8 folds =>  0.879446863378277\n",
            "Average of the accuracy for  9 folds =>  0.8790916068445528\n",
            "\n",
            "Summary of model: SGDClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.8642487046632125\n",
            "Average of the accuracy for  4 folds =>  0.8587205703674835\n",
            "Average of the accuracy for  5 folds =>  0.8687392055267704\n",
            "Average of the accuracy for  6 folds =>  0.8642474850304546\n",
            "Average of the accuracy for  7 folds =>  0.8632161781440654\n",
            "Average of the accuracy for  8 folds =>  0.8656308826005112\n",
            "Average of the accuracy for  9 folds =>  0.8670014125113679\n",
            "\n",
            "Summary of model: KNeighborsClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.7392055267702936\n",
            "Average of the accuracy for  4 folds =>  0.7485332943612786\n",
            "Average of the accuracy for  5 folds =>  0.751986183074266\n",
            "Average of the accuracy for  6 folds =>  0.7537090968445831\n",
            "Average of the accuracy for  7 folds =>  0.7540576535207549\n",
            "Average of the accuracy for  8 folds =>  0.7557879049907409\n",
            "Average of the accuracy for  9 folds =>  0.7561203451085612\n",
            "\n",
            "Summary of model: DecisionTreeClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.8138169257340242\n",
            "Average of the accuracy for  4 folds =>  0.8020687665726751\n",
            "Average of the accuracy for  5 folds =>  0.8172711571675302\n",
            "Average of the accuracy for  6 folds =>  0.8113966134893431\n",
            "Average of the accuracy for  7 folds =>  0.8165805256025279\n",
            "Average of the accuracy for  8 folds =>  0.8189995561745305\n",
            "Average of the accuracy for  9 folds =>  0.8155285952929189\n",
            "\n",
            "Summary of model: SVC()\n",
            "Average of the accuracy for  3 folds =>  0.9053540587219344\n",
            "Average of the accuracy for  4 folds =>  0.9094983876267547\n",
            "Average of the accuracy for  5 folds =>  0.9046632124352332\n",
            "Average of the accuracy for  6 folds =>  0.9074286458825518\n",
            "Average of the accuracy for  7 folds =>  0.9105377842613341\n",
            "Average of the accuracy for  8 folds =>  0.9074212592399871\n",
            "Average of the accuracy for  9 folds =>  0.9088037942162281\n",
            "\n",
            "Summary of model: GaussianNB()\n",
            "Average of the accuracy for  3 folds =>  0.34438687392055267\n",
            "Average of the accuracy for  4 folds =>  0.3443830570902394\n",
            "Average of the accuracy for  5 folds =>  0.34438687392055267\n",
            "Average of the accuracy for  6 folds =>  0.3443918971160536\n",
            "Average of the accuracy for  7 folds =>  0.3443871284696635\n",
            "Average of the accuracy for  8 folds =>  0.34438082520928665\n",
            "Average of the accuracy for  9 folds =>  0.3443926308615459\n",
            "\n",
            "Summary of model: RandomForestClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.8839378238341968\n",
            "Average of the accuracy for  4 folds =>  0.8898041272170132\n",
            "Average of the accuracy for  5 folds =>  0.894300518134715\n",
            "Average of the accuracy for  6 folds =>  0.8925714972981798\n",
            "Average of the accuracy for  7 folds =>  0.8922234086461901\n",
            "Average of the accuracy for  8 folds =>  0.8960185794524111\n",
            "Average of the accuracy for  9 folds =>  0.892907128990022\n",
            "\n",
            "Summary of model: GradientBoostingClassifier()\n",
            "Average of the accuracy for  3 folds =>  0.8880829015544042\n",
            "Average of the accuracy for  4 folds =>  0.8974074986818276\n",
            "Average of the accuracy for  5 folds =>  0.8967184801381693\n",
            "Average of the accuracy for  6 folds =>  0.8949891039463473\n",
            "Average of the accuracy for  7 folds =>  0.899122213365818\n",
            "Average of the accuracy for  8 folds =>  0.8991301403406744\n",
            "Average of the accuracy for  9 folds =>  0.9011994521949825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Program ended!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtixl6_zV3Fn",
        "outputId": "fbc19636-6bce-4579-8e3f-b99573ce57f9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Program ended!\n"
          ]
        }
      ]
    }
  ]
}